{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "This notebook documents the creation of a very simple double deep Q Network model dedicated to trading of SPDR\n",
    "S&P 500 ETF Trust (SPY) stock.\n",
    "\n",
    "The goal of this notebook is to give a very simple code base for beginners to look into and easily understand how\n",
    "it works and hopefully head to an easier understanding of the more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Advances in machine learning (ML) and artificial intelligence (AI) have enabled us to enhance our lives and tackle a variety of\n",
    "complex problems. The financial market is a prime example of a field where researchers are employing these techniques.\n",
    "Since the financial market is very dynamic and ever fluctuating, it presents a unique challenges to consider when\n",
    "developing these systems, but also allows the power of machine learning and AI to shine. Before the development of\n",
    "AI, it was the job of investors and traders to use market data to make optimal decisions that maximize and reduce\n",
    "risk within the context of a trading system. However, due to market complexities, it can be challenging for agents\n",
    "to consider all the relevant information to take an informed position. This is where reinforcement learning (RL),\n",
    "an area of ML, comes into play. Through repeated interaction with a market environment, an RL agent can learn\n",
    "optimal trading strategies by taking certain actions, receiving rewards based on these, and adapting future actions\n",
    "based on previous experience.\n",
    "\n",
    "Reinforcement Learning has a rich history of use in the realm of finance. In the 1990s, Moody and Saffell\n",
    "experimented with real-time recurrent learning in order to demonstrate a predictable structure to U.S. stock\n",
    "prices (Moody & Saffell, 1998). They claimed that their agent was able to make a 4000% profit over the simulated\n",
    "period of 1970 to 1994, far outperforming the S&P 500 stock index during the same timespan.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Description\n",
    "\n",
    "We use one data set for this model. Thie data set was collected from Refinitiv, and it consists of the daily\n",
    "closing prices of SPY, NDAQ.0, DIA, GLD, and USO, although we don't end up using the GLD, and USO data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <mark>For Documentation on Preparing the Program Environment see README.md </mark>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import deque\n",
    "from random import sample\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "Our Model State Space is listed below\n",
    "\n",
    " - 1-Day Return\n",
    " - Previous Action\n",
    " - Previous Price\n",
    " - 2-Day Return\n",
    " - 5-Day Return\n",
    " - 10-Day Return\n",
    " - 21-Day Return\n",
    " - DIA and NDAQ.O's\n",
    "    - 1-Day Return\n",
    "    - 5-Day Return\n",
    "    - 21-Day Return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.638316Z",
     "start_time": "2021-02-25T06:20:29.636097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "trading_cost_bps = .001 #trading cost percent, i.e. .01 is a 1% trading cost\n",
    "time_cost_bps = .0001 #time cost, deducted every day the agent doesn't do something different\n",
    "batch_size = 4096 # for training set to 4096, the batch size to train the NNs on\n",
    "max_episodes = 1000\n",
    "epsilon_decay_steps = 250 # for training set to 250, how many steps it takes epsilon to go from 1 to .01\n",
    "\n",
    "#Random setup stuff\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "#Use a GPU is we have one\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "#Set up results directory\n",
    "results_path = Path('results', 'trading_bot')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Simulation Environment\n",
    "\n",
    "Our environment is a fairly simple market trading simulation. The agent has a choice of three actions:\n",
    "\n",
    ">A = E{0,1,2}, Sell Short, Flat, Buy Long\n",
    "\n",
    "Where\n",
    "* 0: Agent shorts the index fund equal to the amount of possessed capital.\n",
    "* 1: Agent transfers all possessed capital into cash and closes all short positions\n",
    "* 2: Agent buys as much of the given fund as possible with the possessed capital.\n",
    "\n",
    "This is a very simplistic model because the agent cannot invest only a portion of it's capital; it must invest all of its capital or none.\n",
    "\n",
    "At each time step, the simulation updates the portfolio's Net Asset Value (NAV), and performs the agent's chosen action. The NAV is calculated by the following formula:\n",
    "\n",
    "$$\n",
    "NAV_{new} = NAV_{old} * (1 + Reward)\n",
    "$$\n",
    "\n",
    "The function rewarding the agent is simply the percentage change of the NAV. The simulation uses the following equation\n",
    "to calculate the reward function:\n",
    "\n",
    "$$\n",
    "Reward = [(a_{n} - 1) * Return_{n+1}] – trading\\_cost\\_bps*|(a_{n} - 1) - (a_{n-1} - 1)| – dailyCost\n",
    "$$\n",
    "<br>\n",
    "where\n",
    "$$\n",
    "a_{n} = max(neuralNetwork.predict(S_{n})) ∈ {0, 1, 2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dailyCost =\n",
    "  \\begin{cases}\n",
    "    0 &\\text{if } a_{n} = a_{n-1}\\\\\\\n",
    "    \\text{time_cost_bps} &\\text{else }\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Where a<sub>n</sub> is the nth action, S<sub>n</sub> is the nth state, and Return<sub>n</sub> is the 1 day return of the market as found in S<sub>n</sub>.\n",
    "Actions are decremented by 1 so that it translates the action space to -1, 0, and 1. This way, if the agent held cash (now equal to 0), the 1-Day Return will not affect the NAV. If agent bought the stock (now equal\n",
    "to 1), the percent change of NAV will directly correlate to the 1-Day Return. And if the agent instead shorted, the percent change would be inversely correlated to the 1-Day Return.\n",
    "The number of trades are calculated by the equation |(a<sub>n</sub> - 1) - (a<sub>n-1</sub> - 1)|, because if both actions are the same,\n",
    " the agent doesn’t need to trade anything to execute the action. If the agent wants to buy and it previously\n",
    " only held cash, it would only have to buy |0 - 1| = 1 unit of the stock. Whereas if the agent wants to then\n",
    " short, it would first have to sell all the stock it already has and then short that amount again, resulting\n",
    " in |1 - (-1)| = 2 units of trading it would have to do. The amount needed to trade is then multiplied by the\n",
    " trading costs. The daily cost is used to disincentivize the agent from being too passive. If the agent\n",
    " repeats an action, the daily cost is set to the specified value, in our case 0.0001, but if the agent performs\n",
    " a different action, and thus trades something, the dailyCost will be set to 0.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.688161Z",
     "start_time": "2021-02-25T06:20:29.681742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Trading costs: 0.10% | Time costs: 0.01%'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simulation variables\n",
    "trading_days = 252 #days per episode\n",
    "\n",
    "#create the gym environment using the trading_env.py file\n",
    "register(\n",
    "    id='trading-v0',\n",
    "    entry_point='trading_env:TradingEnvironment',\n",
    "    max_episode_steps=trading_days\n",
    "\n",
    ")\n",
    "\n",
    "#print environment variables\n",
    "f'Trading costs: {trading_cost_bps:.2%} | Time costs: {time_cost_bps:.2%}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.544485Z",
     "start_time": "2021-02-25T06:20:29.698083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trading_env:trading_env logger started.\n",
      "INFO:trading_env:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4499 entries, 21 to 4554\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   returns     4499 non-null   float64\n",
      " 1   close       4499 non-null   float64\n",
      " 2   ret_2       4499 non-null   float64\n",
      " 3   ret_5       4499 non-null   float64\n",
      " 4   ret_10      4499 non-null   float64\n",
      " 5   ret_21      4499 non-null   float64\n",
      " 6   NSDQret_1   4499 non-null   float64\n",
      " 7   NSDQret_5   4499 non-null   float64\n",
      " 8   NSDQret_21  4499 non-null   float64\n",
      " 9   DIAret_1    4499 non-null   float64\n",
      " 10  DIAret_5    4499 non-null   float64\n",
      " 11  DIAret_21   4499 non-null   float64\n",
      "dtypes: float64(12)\n",
      "memory usage: 456.9 KB\n"
     ]
    }
   ],
   "source": [
    "#Initalize environment\n",
    "trading_environment = gym.make('trading-v0', trading_days = trading_days)\n",
    "trading_environment.env.trading_days = trading_days\n",
    "trading_environment.env.data_source.trading_days = trading_days\n",
    "trading_environment.env.simulator.steps = trading_days\n",
    "\n",
    "trading_environment.env.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.simulator.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.simulator.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.simulator.reinitialize()\n",
    "trading_environment.seed(42)\n",
    "\n",
    "# Get Environment Params\n",
    "state_dim = len(trading_environment.reset()) #the number of variables in the state space(the number of input nodes)\n",
    "num_actions = trading_environment.action_space.n #the number of actions(the number of output nodes)\n",
    "max_episode_steps = trading_environment.spec.max_episode_steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## Define Trading Agent(he Neural Network)\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_dim,\n",
    "                 num_actions,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 replay_capacity,\n",
    "                 architecture,\n",
    "                 l2_reg,\n",
    "                 tau,\n",
    "                 batch_size):\n",
    "\n",
    "        #initialize\n",
    "        self.state_dim = state_dim #num input nodes\n",
    "        self.num_actions = num_actions #num output nodes\n",
    "        self.experience = deque([], maxlen=replay_capacity) #where to store data from training\n",
    "        self.learning_rate = learning_rate #NN learning rate\n",
    "        self.gamma = gamma #future rewards discount factor\n",
    "        self.architecture = architecture #archetecture of the NN\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.online_network = self.build_model() #The network to train\n",
    "        self.target_network = self.build_model(trainable=False) #the network to keep constant(helps converge quicker)\n",
    "        self.update_target() #set the weights of the target equal to the weights of the online network\n",
    "\n",
    "        self.epsilon = epsilon_start #percent chance to take a random action, vs the optimal action\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps # how many episodes for epsilon to go from epsilon_start to epsilon_end\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps #subtract this from epsilon each episode\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay #multiply epsilon by this each episode after epsilon_decay_steps number of episodes\n",
    "        self.epsilon_history = [] #store the epsilon values\n",
    "\n",
    "        self.total_steps = self.train_steps = 0 #keep track of how many steps we did\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0 #keep rack of the number of episodes\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.batch_size = batch_size #how many data points each round to train the network on\n",
    "        self.tau = tau # how many trainings until we update the target network with the online networks weights\n",
    "        self.losses = []\n",
    "        self.idx = tf.range(batch_size)\n",
    "        self.train = True\n",
    "\n",
    "    # create the neural network with keras\n",
    "    def build_model(self, trainable=True):\n",
    "        layers = []\n",
    "        n = len(self.architecture)\n",
    "        #add the dense layers\n",
    "        for i, units in enumerate(self.architecture, 1):\n",
    "            layers.append(Dense(units=units,\n",
    "                                input_dim=self.state_dim if i == 1 else None, #set the first input to be the dimention of the state space\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=l2(self.l2_reg),\n",
    "                                name=f'Dense_{i}',\n",
    "                                trainable=trainable))\n",
    "        #add a drop out layer\n",
    "        layers.append(Dropout(.1))\n",
    "        #add the final output layer\n",
    "        layers.append(Dense(units=self.num_actions,#number of actions\n",
    "                            trainable=trainable,\n",
    "                            name='Output'))\n",
    "        #initialize the model\n",
    "        model = Sequential(layers)\n",
    "        #add the MSE loss and adam optimizer\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    #set the target network's weight equal to the online network's weights\n",
    "    def update_target(self):\n",
    "        self.target_network.set_weights(self.online_network.get_weights())\n",
    "\n",
    "    #Choose an action based on e greedy\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps += 1\n",
    "        #check if we choose a random action or optimal action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)#return a random action\n",
    "        #return the optimal action\n",
    "        q = self.online_network.predict(state)#predict the q values for each action\n",
    "        return np.argmax(q, axis=1).squeeze()#return the action with the highest Q value\n",
    "\n",
    "    #store the state, action reward, and next state\n",
    "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
    "        #check if its the end of the episode\n",
    "        if not_done:\n",
    "            #it is not so add to the episode\n",
    "            self.episode_reward += r\n",
    "            self.episode_length += 1\n",
    "        else:\n",
    "            #episode ended so decrease epsilon\n",
    "            if self.train:\n",
    "                #check if epsilon needs to be decreased incrementally or exponentially\n",
    "                if self.episodes < self.epsilon_decay_steps:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                else:\n",
    "                    self.epsilon *= self.epsilon_exponential_decay\n",
    "            #add stuff to historic storage\n",
    "            self.episodes += 1\n",
    "            self.rewards_history.append(self.episode_reward)\n",
    "            self.steps_per_episode.append(self.episode_length)\n",
    "            self.episode_reward, self.episode_length = 0, 0\n",
    "        #add the pairs\n",
    "        self.experience.append((s, a, r, s_prime, not_done))\n",
    "\n",
    "    #train the neural networks\n",
    "    def experience_replay(self):\n",
    "        #make sure we have enough data\n",
    "        if self.batch_size > len(self.experience):\n",
    "            return\n",
    "        #Get a batch of data\n",
    "        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n",
    "        states, actions, rewards, next_states, not_done = minibatch\n",
    "        #get the predicted bes actions for the next_states\n",
    "        next_q_values = self.online_network.predict_on_batch(next_states)\n",
    "        best_actions = tf.argmax(next_q_values, axis=1)\n",
    "        #get the predicted reward for the future states from the target network\n",
    "        next_q_values_target = self.target_network.predict_on_batch(next_states)\n",
    "        target_q_values = tf.gather_nd(next_q_values_target,\n",
    "                                       tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n",
    "        #the expected q_values are equal to the observed reward + gamma*the predicted reward of the next state, gamma being the future discount factor\n",
    "        targets = rewards + not_done * self.gamma * target_q_values\n",
    "        #get the predicted q-values, and the store the expected q_values we have among them\n",
    "        q_values = self.online_network.predict_on_batch(states)\n",
    "        q_values[[self.idx, actions]] = targets\n",
    "        #train the network to get q_values when states are passed in, and store the loss\n",
    "        loss = self.online_network.train_on_batch(x=states, y=q_values)\n",
    "        self.losses.append(loss)\n",
    "        #check if we need to set the targets weights to the onlines weights\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.575368Z",
     "start_time": "2021-02-25T06:20:32.565067Z"
    }
   },
   "outputs": [],
   "source": [
    "#RL hypers\n",
    "gamma = .99,  # discount factor\n",
    "tau = 100  # target network update frequency\n",
    "\n",
    "### NN Architecture\n",
    "architecture = (256, 256)  # units per layer\n",
    "learning_rate = 0.0001  # learning rate\n",
    "l2_reg = 1e-6  # L2 regularization\n",
    "\n",
    "### Experience Replay\n",
    "replay_capacity = int(1e6)\n",
    "\n",
    "### epsilon-greedy Policy\n",
    "epsilon_start = 1.0 # starting point for epsilon\n",
    "epsilon_end = .01 # ending point for epsilon\n",
    "epsilon_exponential_decay = .99 # after 250 step(epsilon_decay_steps) epsilon = epsilon*epsilon_exponential_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.613239Z",
     "start_time": "2021-02-25T06:20:32.604766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_1 (Dense)             (None, 256)               3584      \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70,147\n",
      "Trainable params: 70,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#clear out karas\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#instantiate the ddqn model\n",
    "ddqn = DDQNAgent(state_dim=state_dim,\n",
    "                 num_actions=num_actions,\n",
    "                 learning_rate=learning_rate,\n",
    "                 gamma=gamma,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                 replay_capacity=replay_capacity,\n",
    "                 architecture=architecture,\n",
    "                 l2_reg=l2_reg,\n",
    "                 tau=tau,\n",
    "                 batch_size=batch_size)\n",
    "\n",
    "ddqn.online_network.summary()\n",
    "\n",
    "### Set Experiment parameters\n",
    "\n",
    "total_steps = 0\n",
    "#max_episodes = 20\n",
    "\n",
    "### Initialize Experiment variables\n",
    "# these store all the data we want to keep track of\n",
    "# navs - agent's net asset values for each episode\n",
    "# market_navs- market's net asset values for each episode\n",
    "# diffs- navs - market_navs\n",
    "# holds - the number of times the agent held\n",
    "# shorts - the number of times the agent shorted\n",
    "# buys - the number of times the agent bought\n",
    "episode_time, navs, market_navs, diffs, holds, shorts, buys = [], [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.752721Z",
     "start_time": "2021-02-25T06:20:32.742471Z"
    }
   },
   "outputs": [],
   "source": [
    "#prints the results from the training and testing runs\n",
    "def track_results(episode, nav_ma_100, nav_ma_10,\n",
    "                  market_nav_100, market_nav_10,\n",
    "                  win_ratio, total, epsilon):\n",
    "    time_ma = np.mean([episode_time[-100:]])\n",
    "    T = np.sum(episode_time)\n",
    "\n",
    "    #set up the format to print to\n",
    "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
    "    print(\"Training Results:\" + template.format(episode, format_time(total),\n",
    "                          nav_ma_100-1, nav_ma_10-1, \n",
    "                          market_nav_100-1, market_nav_10-1, \n",
    "                          win_ratio, epsilon))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1\n",
      "Episode:  2\n",
      "Episode:  3\n",
      "Episode:  4\n",
      "Episode:  5\n",
      "Episode:  6\n",
      "Episode:  7\n",
      "Episode:  8\n",
      "Episode:  9\n",
      "Episode:  10\n",
      "Training Results:  10 | 00:00:03 | Agent: -18.2% (-18.2%) | Market:   5.9% (  5.9%) | Wins: 20.0% | eps:  0.960\n",
      "Episode:  11\n",
      "Episode:  12\n",
      "Episode:  13\n",
      "Episode:  14\n",
      "Episode:  15\n",
      "Episode:  16\n",
      "Episode:  17\n",
      "Episode:  18\n",
      "Episode:  19\n",
      "Episode:  20\n",
      "Training Results:  20 | 00:01:13 | Agent: -18.4% (-18.6%) | Market:  10.0% ( 14.1%) | Wins: 10.0% | eps:  0.921\n",
      "final\n",
      "Training Results:  20 | 00:01:13 | Agent: -18.4% (-18.6%) | Market:  10.0% ( 14.1%) | Wins: 10.0% | eps:  0.921\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []\n",
    "for episode in range(1, max_episodes + 1):\n",
    "    #reset the environment and get the first state\n",
    "    this_state = trading_environment.reset()\n",
    "    numBuy = 0\n",
    "    numShort = 0\n",
    "    numHold = 0\n",
    "    print(\"Episode: \", episode)\n",
    "    #loop for a year\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        #get the action chosen by the agent\n",
    "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "\n",
    "\n",
    "        #execute the action and get the results\n",
    "        next_state, reward, done, _ = trading_environment.step(action)\n",
    "        #store the action\n",
    "        if action == 0:\n",
    "            numShort += 1\n",
    "        elif action == 1:\n",
    "            numHold += 1\n",
    "        else:\n",
    "            numBuy += 1\n",
    "        #store the results\n",
    "        ddqn.memorize_transition(this_state, \n",
    "                                 action, \n",
    "                                 reward, \n",
    "                                 next_state, \n",
    "                                 0.0 if done else 1.0)#whether or not the episode is over\n",
    "        #train the network\n",
    "        if ddqn.train:\n",
    "            ddqn.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "        #increment state\n",
    "        this_state = next_state\n",
    "\n",
    "\n",
    "\n",
    "    # get DataFrame with seqence of actions, returns and nav values\n",
    "    result = trading_environment.env.simulator.result()\n",
    "    \n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # get nav\n",
    "    nav = final.nav\n",
    "    navs.append(nav)\n",
    "\n",
    "    # market nav \n",
    "    market_nav = final.market_nav\n",
    "    market_navs.append(market_nav)\n",
    "\n",
    "    #num holds buys and sells\n",
    "    holds.append(numHold)\n",
    "    buys.append(numBuy)\n",
    "    shorts.append(numShort)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    diff = nav - market_nav\n",
    "    diffs.append(diff)\n",
    "    if episode % 10 == 0:\n",
    "        track_results(episode, \n",
    "                      # show mov. average results for 100 (10) periods\n",
    "                      np.mean(navs[-100:]), \n",
    "                      np.mean(navs[-10:]), \n",
    "                      np.mean(market_navs[-100:]), \n",
    "                      np.mean(market_navs[-10:]), \n",
    "                      # share of agent wins, defined as higher ending nav\n",
    "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100), \n",
    "                      time() - start, ddqn.epsilon)\n",
    "\n",
    "    #stop if the agent beat the market 25 times in a row, it is good enough\n",
    "    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n",
    "        print(result.tail())\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"final\")\n",
    "track_results(episode,\n",
    "              # show mov. average results for 100 (10) periods\n",
    "              np.mean(navs[-100:]),\n",
    "              np.mean(navs[-10:]),\n",
    "              np.mean(market_navs[-100:]),\n",
    "              np.mean(market_navs[-10:]),\n",
    "              # share of agent wins, defined as higher ending nav\n",
    "              np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
    "              time() - start, ddqn.epsilon)\n",
    "\n",
    "trading_environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Results\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.020Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(len(diffs))\n",
    "#put the data in a dataframe\n",
    "results = pd.DataFrame({'NumStateVars': state_dim,\n",
    "                            'TradeCost': trading_cost_bps,\n",
    "                            'TimeCost': time_cost_bps,\n",
    "                            'EpsilonSteps': epsilon_decay_steps,\n",
    "                            'Episode': list(range(1, episode+1)),\n",
    "                            'TrainAgent': navs,\n",
    "                            'TrainMarket': market_navs,\n",
    "                            'TrainDifference': diffs,\n",
    "                            'Holds': holds,\n",
    "                            'Buys': buys,\n",
    "                            'Shorts': shorts}).set_index('Episode')\n",
    "#get the win percent of the past 100 episodes\n",
    "results['Strategy Wins (%)'] = (results.TrainDifference > 0).rolling(100).sum()\n",
    "results.info()\n",
    "\n",
    "#Get the date and time so we can keep track of the data files\n",
    "currentTime = datetime.now()\n",
    "training_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TrainResults.csv'\n",
    "\n",
    "\n",
    "#store the results in a csv\n",
    "results.to_csv(results_path / training_file_name)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20 entries, 1 to 20\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   NumStateVars       20 non-null     int64  \n",
      " 1   TradeCost          20 non-null     float64\n",
      " 2   TimeCost           20 non-null     float64\n",
      " 3   EpsilonSteps       20 non-null     int64  \n",
      " 4   TrainAgent         20 non-null     float64\n",
      " 5   TrainMarket        20 non-null     float64\n",
      " 6   TrainDifference    20 non-null     float64\n",
      " 7   Holds              20 non-null     int64  \n",
      " 8   Buys               20 non-null     int64  \n",
      " 9   Shorts             20 non-null     int64  \n",
      " 10  Strategy Wins (%)  0 non-null      float64\n",
      "dtypes: float64(6), int64(5)\n",
      "memory usage: 1.9 KB\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}