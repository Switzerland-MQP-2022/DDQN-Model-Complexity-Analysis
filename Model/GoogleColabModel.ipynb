{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "This notebook documents the analysis of a variety of reinforcement learning models dedicated to trading of SPDR S&P 500 ETF Trust (SPY) stock.  These models all use the same algorithms and structure, a Double Deep Q Learning model, but differ in their state space, or environment.\n",
    "\n",
    "The goal of this project is to analyze how increasing the\n",
    "complexity of a model's state space affects the model's performance. This is an interesting topic because naturally state spaces may be too simple to properly learn, but due to the \"Curse of Dimensionality,\" if a state space gets too complicated, we can expect the model to overfit and possibly suffer worse performance.\n",
    "\n",
    "This notebook stores each of the state spaces we experimented with, and allows a user to select a state space to train themselves due to the long processing time required to train each state space at once."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Advances in machine learning (ML) and artificial intelligence (AI) have enabled us to enhance our lives and tackle a variety of\n",
    "complex problems. The financial market is a prime example of a field where researchers are employing these techniques. Since the financial market is very dynamic and ever fluctuating, it presents a unique challenges to consider when developing these systems, but also allows the power of machine learning and AI to shine. Before the development of AI, it was the job of investors and traders to use market data to make optimal decisions that maximize and reduce risk within the context of a trading system. However, due to market complexities, it can be challenging for agents to consider all the relevant information to take an informed position. This is where reinforcement learning (RL), an area of ML, comes into play. Through repeated interaction with a market environment, an RL agent can learn optimal trading strategies by taking certain actions, receiving rewards based on these, and adapting future actions based on previous experience.\n",
    "\n",
    "Reinforcement Learning has a rich history of use in the realm of finance. In the 1990s, Moody and Saffell experimented with real-time recurrent learning in order to demonstrate a predictable structure to U.S. stock prices (Moody & Saffell, 1998). They claimed that their agent was able to make a 4000% profit over the simulated period of 1970 to 1994, far outperforming the S&P 500 stock index during the same timespan.\n",
    "\n",
    "However, previous studies into applying reinforcement learning into finance have provided insufficient analysis of their chosen model compared to similar ones. For instance, Wu et al. came up with their own technical indicators to add to their reinforcement model [233]. However, they did not test their model against simpler models, they only tested it against the turtle trading strategy [256], a simple rule-based strategy. This is an issue due to the well-studied phenomenon known as the “curse of\n",
    "dimensionality.” Simply put, as one adds more dimensions to a dataset with a fixed number of data points, the density of the data points gets smaller and thus it becomes harder to prevent models from overfitting. Somewhat paradoxically, this could lead to more complex models performing worse than simpler ones. Thus, it is important to test the model on multiple dimensionalities of data, to make sure the data is not too complex that it overfits,\n",
    "or too simple that it can’t learn enough.\n",
    "\n",
    "Since these papers do not provide an in-depth analysis, this notebook analyses how altering the complexity of data available to a trading agent affects its overall performance relative to the market. To do this, this notebook adopts a DDQN algorithm to trade in three environments, each focusing on one of equity indices, foreign exchange (Forex), and market3. Each market environment contains multiple state spaces with varying amounts of data and asset dimensionality, such as 1-Day returns, 5-Day returns, currencies and market3example. The user can then decide which dataset and state space to train, thus seeing how well each model performs, and which amount of dimentionality is the best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <mark>Prepare the Program Environment</mark>\n",
    "1) Update the \"tables\" package, then restart the runtime\n",
    "\n",
    "2) Upload the IndexFundsData.csv file into colab\n",
    "\n",
    "3) Upload the training_env.py file\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install --user --upgrade tables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Description\n",
    "\n",
    "We use three data sets for our models: one for equity indexes; one for the foreign exchange market;\n",
    "and one for _____. These data sets were collected from Refinitiv and Yahoo Finance, and they consist of the daily closing prices of\n",
    "their respective assets.\n",
    "\n",
    "For the equity indices, we have the prices of SPY (the index we are predicting), as well as the prices for NSDQ.O, DIA, GLD, and USO. We are using NSDQ.O and DIA as they are similar indices that could reasonably help to predict SPY. This data is used in our first and second most complex environments. We are then using GLD and USO as they are further removed from the SPY and actually make the model preform worse, thus showing the curse of dimensionality. That data is only used in our most complex environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports & Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set Path and Read CSV File"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    DATA_STORE = Path('IndexAssets.h5')\n",
    "    df = (pd.read_csv('IndexFundsData.csv'))\n",
    "    label = 'SAP'\n",
    "\n",
    "except:\n",
    "    DATA_STORE = Path('FXAssets.h5')\n",
    "    df = (pd.read_csv('FXData.csv'))\n",
    "    label = 'FX'\n",
    "\n",
    "print(df.head(10))#make sure we got the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Store Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put(label, df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Connect and Store to Google Drive\n",
    "\n",
    "Allow this notebook to access your Google Drive when prompted, as that is where the data will be stored\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import deque\n",
    "from random import sample\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "\n",
    "# conect to google drive so we can store data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Methodology\n",
    "\n",
    "Since we are comparing the effectiveness of data with different dimensionalities we naturally have to train multiple models. The state variables for each model is shown in the table below.\n",
    "\n",
    "| Model 1 | Model 2 | Model 3 | Model 4 | Model 5 | Model 6 |\n",
    "|-|-|-|-|-|-|\n",
    "| 1-Day Return| Model 1 Vars.<br><br>Previous Action | Model 2 Vars.<br><br>Previous Price | Model 3 Vars.<br><br>2-Day Return<br><br>5-Day Return<br><br>10-Day Return<br><br>21-Day Return | Model 4 Vars.<br><br>2 Similar Indexes'<br>· 1-Day Return<br>· 5-Day Return<br>· 21-Day Return | Model 5 Vars.<br><br>2 Unconnected Indexes'<br>· 1-Day Return<br>· 5-Day Return<br>· 21-Day Return |\n",
    "\n",
    "This notebook allows you to specify which state space you want to use, as training all of them at once could be very time-consuming."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Settings\n",
    "<mark> Select which model you want to run by setting the model variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Which model to run(0 - 6, 0 being the simplest, 6 being the most complex)\n",
    "model = 0\n",
    "\n",
    "whenSave = 10 # after how many episodes should the csv be saved to google drive\n",
    "stopAfterOne = True # only run 1 episode\n",
    "printStep = True # print out stuff for each step(used with stopAfterOne, becasue running more than\n",
    "#one episode will print a lot of stuff)\n",
    "trading_cost_bps = 0\n",
    "time_cost_bps = 0\n",
    "batch_size = 256 # for training set to 4096\n",
    "max_episodes = 20\n",
    "epsilon_decay_steps = max_episodes/2 # for training set to 250\n",
    "\n",
    "\n",
    "#Random setup stuff\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "#Use a GPU is we have one\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "#Set up results directory to google drive\n",
    "results_path = \"/content/gdrive/My Drive/\"\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Simulation Environment\n",
    "\n",
    "Our environment is a fairly simple market trading simulation. The agent has a choice of three actions:\n",
    "\n",
    ">A = E{0,1,2}, Sell Short, Flat, Buy Long\n",
    "\n",
    "Where\n",
    "* 0: Agent shorts the index fund equal to the amount of possessed capital.\n",
    "* 1: Agent transfers all possessed capital into cash and closes all short positions\n",
    "* 2: Agent buys as much of the given fund as possible with the possessed capital.\n",
    "\n",
    "This is a very simplistic model because the agent cannot invest only a portion of it's capital; it must invest all of its capital or none.\n",
    "\n",
    "At each time step, the simulation updates the portfolio's Net Asset Value (NAV), and performs the agent's chosen action. The NAV is calculated by the following formula:\n",
    "\n",
    "$$\n",
    "NAV_{new} = NAV_{old} * (1 + Reward)\n",
    "$$\n",
    "\n",
    "The function rewarding the agent is simply the percentage change of the NAV. The simulation uses the following equation to calculate the reward function:\n",
    "\n",
    "$$\n",
    "Reward = [(Action_{Yesterday} - 1) * Return_{1 Day}] – Trading Costs – Daily Cost\n",
    "$$\n",
    "<br>\n",
    "where\n",
    "$$\n",
    "Trading Costs = 0.001 * |Action_{Today} - Action_{Yesterday}|,\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Daily Cost = 0.0001\n",
    "$$\n",
    "\n",
    "Yesterday's action is decremented by 1 so that it translates the action space to -1, 0, and 1. This way, if the agent held cash (now equal to 0), the 1-Day Return will not affect the NAV. If agent bought the stock (now equal\n",
    "to 1), the percent change of NAV will directly correlate to the 1-Day Return. And if the agent instead shorted, the percent change would be inversely correlated to the 1-Day Return.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Create and Initialize Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Simulation variables\n",
    "trading_days = 252\n",
    "#trading_cost_bps = 1e-3\n",
    "#time_cost_bps = 1e-4\n",
    "\n",
    "\n",
    "register(\n",
    "    id='trading-v0',\n",
    "    entry_point='trading_env:TradingEnvironment',\n",
    "    max_episode_steps=trading_days\n",
    ")\n",
    "\n",
    "f'Trading costs: {trading_cost_bps:.2%} | Time costs: {time_cost_bps:.2%}'\n",
    "\n",
    "#Initalize environment\n",
    "trading_environment = gym.make('trading-v0', trading_days = trading_days, model = model)\n",
    "trading_environment.env.trading_days = trading_days\n",
    "trading_environment.env.data_source.trading_days = trading_days\n",
    "trading_environment.env.simulator.steps = trading_days\n",
    "\n",
    "trading_environment.env.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.simulator.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.simulator.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.simulator.reinitialize()\n",
    "trading_environment.seed(42)\n",
    "\n",
    "# Get Environment Params\n",
    "state_dim = len(trading_environment.reset())\n",
    "num_actions = trading_environment.action_space.n\n",
    "max_episode_steps = trading_environment.spec.max_episode_steps\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Define Trading Agent(the Neural Network)\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_dim,\n",
    "                 num_actions,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 replay_capacity,\n",
    "                 architecture,\n",
    "                 l2_reg,\n",
    "                 tau,\n",
    "                 batch_size):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.experience = deque([], maxlen=replay_capacity)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.architecture = architecture\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.online_network = self.build_model()\n",
    "        self.target_network = self.build_model(trainable=False)\n",
    "        self.update_target()\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
    "        self.epsilon_history = []\n",
    "\n",
    "        self.total_steps = self.train_steps = 0\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.losses = []\n",
    "        self.idx = tf.range(batch_size)\n",
    "        self.train = True\n",
    "\n",
    "    def build_model(self, trainable=True):\n",
    "        layers = []\n",
    "        n = len(self.architecture)\n",
    "        for i, units in enumerate(self.architecture, 1):\n",
    "            layers.append(Dense(units=units,\n",
    "                                input_dim=self.state_dim if i == 1 else None,\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=l2(self.l2_reg),\n",
    "                                name=f'Dense_{i}',\n",
    "                                trainable=trainable))\n",
    "        layers.append(Dropout(.1))\n",
    "        layers.append(Dense(units=self.num_actions,\n",
    "                            trainable=trainable,\n",
    "                            name='Output'))\n",
    "        model = Sequential(layers)\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_network.set_weights(self.online_network.get_weights())\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps += 1\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        q = self.online_network.predict(state)\n",
    "        return np.argmax(q, axis=1).squeeze()\n",
    "\n",
    "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
    "        if not_done:\n",
    "            self.episode_reward += r\n",
    "            self.episode_length += 1\n",
    "        else:\n",
    "            if self.train:\n",
    "                if self.episodes < self.epsilon_decay_steps:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                else:\n",
    "                    self.epsilon *= self.epsilon_exponential_decay\n",
    "\n",
    "            self.episodes += 1\n",
    "            self.rewards_history.append(self.episode_reward)\n",
    "            self.steps_per_episode.append(self.episode_length)\n",
    "            self.episode_reward, self.episode_length = 0, 0\n",
    "\n",
    "        self.experience.append((s, a, r, s_prime, not_done))\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if self.batch_size > len(self.experience):\n",
    "            return\n",
    "        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n",
    "        states, actions, rewards, next_states, not_done = minibatch\n",
    "\n",
    "        next_q_values = self.online_network.predict_on_batch(next_states)\n",
    "        best_actions = tf.argmax(next_q_values, axis=1)\n",
    "\n",
    "        next_q_values_target = self.target_network.predict_on_batch(next_states)\n",
    "        target_q_values = tf.gather_nd(next_q_values_target,\n",
    "                                       tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n",
    "\n",
    "        targets = rewards + not_done * self.gamma * target_q_values\n",
    "\n",
    "        q_values = self.online_network.predict_on_batch(states)\n",
    "        q_values[[self.idx, actions]] = targets\n",
    "\n",
    "        loss = self.online_network.train_on_batch(x=states, y=q_values)\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reinforcement Learning parameters\n",
    "\n",
    "gamma = .99,  # discount factor\n",
    "tau = 100  # target network update frequency\n",
    "\n",
    "# Neural Network Architecture\n",
    "\n",
    "architecture = (256, 256)  # units per layer\n",
    "learning_rate = 0.0001  # learning rate\n",
    "l2_reg = 1e-6  # L2 regularization\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "replay_capacity = int(1e6)\n",
    "#batch_size = 4096\n",
    "\n",
    "### epsilon-greedy Policy\n",
    "\n",
    "epsilon_start = 1.0 # starting point for epsilon\n",
    "epsilon_end = .01 # ending point for epsilon\n",
    "#epsilon_decay_steps = 250 # the number of steps to get from start to end\n",
    "\n",
    "epsilon_exponential_decay = .99 # after 250 step(epsilon_decay_steps) epsilon = epsilon*epsilon_exponential_decay"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create DDQN Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clear out karas\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Instantiate the DDQN model\n",
    "ddqn = DDQNAgent(state_dim=state_dim,\n",
    "                 num_actions=num_actions,\n",
    "                 learning_rate=learning_rate,\n",
    "                 gamma=gamma,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                 replay_capacity=replay_capacity,\n",
    "                 architecture=architecture,\n",
    "                 l2_reg=l2_reg,\n",
    "                 tau=tau,\n",
    "                 batch_size=batch_size)\n",
    "\n",
    "ddqn.online_network.summary()\n",
    "\n",
    "### Set Experiment parameters\n",
    "\n",
    "total_steps = 0\n",
    "#max_episodes = 1000\n",
    "\n",
    "### Initialize Experiment variables\n",
    "\n",
    "episode_time, navs, market_navs, diffs, episode_eps, holds, shorts, buys = [], [], [], [], [], [], [], []\n",
    "test_navs, test_market_navs, test_diffs, test_holds, test_shorts, test_buys = [], [], [], [], [], []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Prints the results from the training and testing runs\n",
    "\n",
    "def track_results(episode, nav_ma_100, nav_ma_10,\n",
    "                  market_nav_100, market_nav_10,\n",
    "                  win_ratio, total, epsilon, pretext=\"Training Results:\"):\n",
    "    time_ma = np.mean([episode_time[-100:]])\n",
    "    T = np.sum(episode_time)\n",
    "\n",
    "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
    "    print(pretext + template.format(episode, format_time(total),\n",
    "                          nav_ma_100-1, nav_ma_10-1,\n",
    "                          market_nav_100-1, market_nav_10-1,\n",
    "                          win_ratio, epsilon))\n",
    "\n",
    "#Runs a year long simulation on the testing data\n",
    "def test_data_simulation():\n",
    "    #reset the environment\n",
    "    testthis_state = trading_environment.reset(training=False)\n",
    "    num_holds = 0\n",
    "    num_buys = 0\n",
    "    num_shorts = 0\n",
    "\n",
    "    #loop for a year\n",
    "\n",
    "    for test_episode_step in range(max_episode_steps):\n",
    "        testaction = ddqn.epsilon_greedy_policy(testthis_state.reshape(-1, state_dim))\n",
    "        testnext_state, testreward, testdone, _ = trading_environment.step(testaction)\n",
    "\n",
    "        if testaction == 0:\n",
    "            num_shorts += 1\n",
    "        elif testaction == 1:\n",
    "            num_holds += 1\n",
    "        else:\n",
    "            num_buys += 1\n",
    "\n",
    "\n",
    "        if testdone:\n",
    "            break\n",
    "        testthis_state = testnext_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_holds.append(num_holds)\n",
    "    test_shorts.append(num_shorts)\n",
    "    test_buys.append(num_buys)\n",
    "\n",
    "    # get DataFrame with sequence of actions, returns, and NAV values\n",
    "    test_result = trading_environment.env.simulator.result()\n",
    "\n",
    "    # get results of last step\n",
    "    test_final = test_result.iloc[-1]\n",
    "\n",
    "\n",
    "    # get nav\n",
    "    test_nav = test_final.nav\n",
    "    test_navs.append(test_nav)\n",
    "\n",
    "    # market nav\n",
    "    test_market_nav = test_final.market_nav\n",
    "    test_market_navs.append(test_market_nav)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    test_diff = test_nav - test_market_nav\n",
    "    test_diffs.append(test_diff)\n",
    "\n",
    "    #Store the results\n",
    "    track_results(episode,\n",
    "                  # show mov. average results for 100 (10) periods\n",
    "                  np.mean(test_navs[-100:]),\n",
    "                  np.mean(test_navs[-10:]),\n",
    "                  np.mean(test_market_navs[-100:]),\n",
    "                  np.mean(test_market_navs[-10:]),\n",
    "                  # share of agent wins, defined as higher ending NAV\n",
    "                  np.sum([s > 0 for s in test_diffs[-100:]])/min(len(test_diffs), 100),\n",
    "                  time() - start, -1, pretext=\"Testing Results:\")\n",
    "\n",
    "\n",
    "def saveData():\n",
    "    print(len(diffs))\n",
    "\n",
    "    exampleState = trading_environment.reset()\n",
    "    numStateVars = len(exampleState)\n",
    "\n",
    "    results = pd.DataFrame({'NumStateVars': numStateVars,\n",
    "                            'TradeCost': trading_cost_bps,\n",
    "                            'TimeCost': time_cost_bps,\n",
    "                            'EpsilonSteps': epsilon_decay_steps,\n",
    "                            'Episode': list(range(1, episode+1)),\n",
    "                            'TrainAgent': navs,\n",
    "                            'TrainMarket': market_navs,\n",
    "                            'TrainDifference': diffs,\n",
    "                            'Holds': holds,\n",
    "                            'Buys': buys,\n",
    "                            'Shorts': shorts}).set_index('Episode')\n",
    "\n",
    "    results['Strategy Wins (%)'] = (results.TrainDifference > 0).rolling(100).sum()\n",
    "\n",
    "\n",
    "    test_results = pd.DataFrame({'NumStateVars': numStateVars,\n",
    "                            'TradeCost': trading_cost_bps,\n",
    "                            'TimeCost': time_cost_bps,\n",
    "                            'EpsilonSteps': epsilon_decay_steps,\n",
    "                            'EpisodeDiv10': list(range(1, len(test_navs)+1)),\n",
    "                            'TestAgent': test_navs,\n",
    "                            'TestMarket': test_market_navs,\n",
    "                            'TestDifference': test_diffs,\n",
    "                            'Holds': test_holds,\n",
    "                            'Buys': test_buys,\n",
    "                            'Shorts': test_shorts}).set_index('EpisodeDiv10')\n",
    "\n",
    "\n",
    "    test_results['Strategy Wins (%)'] = (test_results.TestDifference > 0).rolling(100).sum()\n",
    "\n",
    "    # get the date and time so we can keep an ordered record of the data files\n",
    "    currentTime = datetime.now()\n",
    "    training_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TrainResults.csv'\n",
    "    testing_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TestResults.csv'\n",
    "\n",
    "\n",
    "    # store the results in a csv file\n",
    "    results.to_csv(results_path + training_file_name)\n",
    "    test_results.to_csv(results_path + testing_file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Agent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time()\n",
    "results = []\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"model: \", model)\n",
    "print(\"whenSave: \", whenSave)\n",
    "print(\"trading_cost_bps: \", trading_cost_bps)\n",
    "print(\"time_cost_bps: \", time_cost_bps)\n",
    "print(\"batch_size: \", batch_size)\n",
    "print(\"max_episodes: \", max_episodes)\n",
    "print(\"epsilon_decay_steps: \", epsilon_decay_steps)\n",
    "print(\"-----------------------------------------------\")\n",
    "for episode in range(1, max_episodes + 1):\n",
    "    this_state = trading_environment.reset()\n",
    "    numBuy = 0\n",
    "    numShort = 0\n",
    "    numHold = 0\n",
    "    print(\"Episode: \", episode)\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "\n",
    "        if action == 0:\n",
    "            numShort += 1\n",
    "        elif action == 1:\n",
    "            numHold += 1\n",
    "        else:\n",
    "            numBuy += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        next_state, reward, done, _ = trading_environment.step(action)\n",
    "\n",
    "        ddqn.memorize_transition(this_state,\n",
    "                                 action,\n",
    "                                 reward,\n",
    "                                 next_state,\n",
    "                                 0.0 if done else 1.0)\n",
    "        if printStep:\n",
    "            print(\"Step: \", episode_step)\n",
    "            print(\"ts: \", this_state)\n",
    "            print(\"ac: \", action)\n",
    "            print(\"re: \", reward)\n",
    "            print(\"ns: \", next_state)\n",
    "\n",
    "        if ddqn.train:\n",
    "            ddqn.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "        this_state = next_state\n",
    "\n",
    "    if stopAfterOne:\n",
    "        break\n",
    "\n",
    "\n",
    "    # get DataFrame with sequence of actions, returns and NAV values\n",
    "    result = trading_environment.env.simulator.result()\n",
    "\n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # get NAV\n",
    "    nav = final.nav\n",
    "    navs.append(nav)\n",
    "\n",
    "    # market NAV\n",
    "    market_nav = final.market_nav\n",
    "    market_navs.append(market_nav)\n",
    "\n",
    "    #num holds buys and sells\n",
    "    holds.append(numHold)\n",
    "    buys.append(numBuy)\n",
    "    shorts.append(numShort)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    diff = nav - market_nav\n",
    "    diffs.append(diff)\n",
    "    if episode % 10 == 0:\n",
    "        track_results(episode,\n",
    "                      # show mov. average results for 100 (10) periods\n",
    "\n",
    "                      np.mean(navs[-100:]),\n",
    "                      np.mean(navs[-10:]),\n",
    "                      np.mean(market_navs[-100:]),\n",
    "                      np.mean(market_navs[-10:]),\n",
    "                      # share of agent wins, defined as higher ending NAV\n",
    "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
    "                      time() - start, ddqn.epsilon)\n",
    "        test_data_simulation()\n",
    "    if episode % whenSave == 0:\n",
    "        saveData()\n",
    "    #Stop training if the last 25 episodes did better than the market\n",
    "    #if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n",
    "    #    print(result.tail())\n",
    "    #    break\n",
    "\n",
    "if not stopAfterOne:\n",
    "    print(\"final\")\n",
    "    track_results(episode,\n",
    "                  # show mov. average results for 100 (10) periods\n",
    "                  np.mean(navs[-100:]),\n",
    "                  np.mean(navs[-10:]),\n",
    "                  np.mean(market_navs[-100:]),\n",
    "                  np.mean(market_navs[-10:]),\n",
    "                  # share of agent wins, defined as higher ending nav\n",
    "                  np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
    "                  time() - start, ddqn.epsilon)\n",
    "    test_data_simulation()\n",
    "    trading_environment.close()\n",
    "    saveData()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Store Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "saveData()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# plot density histogram\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    sns.distplot(results.TrainDifference)\n",
    "    sns.despine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title\n",
    "# plot annual returns and agent outperformance line graphs\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "df1 = (results[['TrainAgent', 'TrainMarket']]\n",
    "       .sub(1)\n",
    "       .rolling(100)\n",
    "       .mean())\n",
    "df1.plot(ax=axes[0],\n",
    "         title='Annual Returns (Moving Average)',\n",
    "         lw=1)\n",
    "\n",
    "df2 = results['Strategy Wins (%)'].div(100).rolling(50).mean()\n",
    "df2.plot(ax=axes[1],\n",
    "         title='Agent Outperformance (%, Moving Average)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.yaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))\n",
    "axes[1].axhline(.5, ls='--', c='k', lw=1)\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'performance', dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}