{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook documents the analysis of a variety of reinforcement learning models dedicated to SPDR\n",
    "trading.  These models all use the same algorithms and structure, a Double Deep Q Learning model, but where\n",
    "they differ is in the state space, or environment.  This projects goal is to analyse how increasing the\n",
    "complexity of the model's state space affects the performance of the model. This is an interesting topic\n",
    "because naturally state spaces can be too simple and not be able to learn, but due to the curse of\n",
    "dimensionality, if a state space gets too complicated we expect the model to over fit and perform\n",
    "poorly again.  This notebook stores all the different state spaces we tried and allow the user to select a\n",
    "state space to train themselves. We are letting the use select which one to train because training 6 models\n",
    "all at once will take days."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "Advances in machine learning and artificial intelligence (AI) have enabled us to enhance our lives and tackle a variety of\n",
    "complex problems. The financial market is a prime example of a field where researchers are employing these\n",
    "techniques. Since the financial market is very dynamic and ever fluctuating, it presents a unique challenges to consider\n",
    "when developing these systems, but also allows the power of machine learning and AI to shine. Before the\n",
    "development of AI, it was the job of investors and traders to use market data to make optimal\n",
    "decisions that maximize and reduce risk within the context of a trading system. However, due to market\n",
    "complexities, it can be challenging for agents to consider all the relevant information to take an informed\n",
    "position. This is where reinforcement learning (RL), an area of machine learning, comes into play. Through\n",
    "repeated interaction with a market environment, an RL agent can learn optimal trading strategies by taking\n",
    "certain actions, receiving rewards based on these, and adapting future actions based on previous experience.\n",
    "\n",
    "Reinforcement Learning has a rich history of use in the realm of finance. In the 1990s, Moody and Saffell experimented\n",
    "with real-time recurrent learning in order to demonstrate a predictable structure to U.S. stock prices (Moody\n",
    "& Saffell, 1998). They claimed that their agent was able to make a 4000% profit over the simulated period of\n",
    "1970 to 1994, far outperforming the S&P 500 stock index during the same timespan.\n",
    "\n",
    "However, previous studies into applying reinforcement learning into finance have provided insufficient analysis\n",
    "of their chosen model compared to similar ones. For instance, Wu et al. came up with their own technical\n",
    "indicators to add to their reinforcement model  [233]. However, they did not test their model against simpler\n",
    "models, they only tested it against the turtle trading strategy[256], a simple rule based strategy.\n",
    "This is an issue due to the well-studied phenomenon known as the “curse of\n",
    "dimensionality.” Simply put, as one adds more dimensions to a dataset with a fixed number of data points, the\n",
    "density of the data points gets smaller and thus it becomes harder to prevent models from overfitting. Somewhat\n",
    "paradoxically, this could lead to more complex models performing worse than simpler ones. Thus, it is important\n",
    "to test the model on multiple dimensionalities of data, to make sure the data is not too complex that it overfits,\n",
    "or too simple that it can’t learn enough.\n",
    "\n",
    "Since these papers do not provide an in-depth analysis, this notebook analyses how altering\n",
    "the complexity of data available to a trading agent affects its overall performance relative to the market. To\n",
    "do this, this notebook adopts a DDQN algorithm to trade in three environments, each focusing on one of equity indices, foreign\n",
    "exchange (Forex), and market3. Each market environment contains multiple state spaces with varying amounts of data\n",
    "and asset dimensionality, such as 1-Day returns, 5-Day returns, currencies and market3example.\n",
    " The user can then decide which dataset and state\n",
    "space to train, thus seeing how well each model performs, and which amount of dimentionality is the best.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <mark>Set up Environment </mark>\n",
    "1) update tables, then restart the runtime\n",
    "\n",
    "2) upload the IndexFundsData.csv file into colab\n",
    "\n",
    "3) upload the training_env.py file\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install --user --upgrade tables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Description\n",
    "\n",
    "There are 3 data sets we are using for these models, one for equity indexes, one for the foreign exchange market,\n",
    "and one for _____.  These data sets were collected from Refinitiv, and they consist of the daily closing prices of\n",
    "each asset we were using.  For the equity indexes we naturally have the daily close prices of SPY, as that is the\n",
    "index we are predicting, we also have the daily close prices of NSDQ.O, DIA, GLD, and USO.  We are using NSDQ.O and\n",
    "DIA as they are similar to the SPY and could reasonably help predict SPY, this data is used in the 4th and 5th complex\n",
    "models.  We are using GLD and USO as they are further removed and actually do more harm than good and make the model\n",
    "preform worse, thus showing the curse of dimensionality.  That data is only used in the 5th model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collect data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set Data Store path and Get data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_STORE = Path('IndexAssets.h5')\n",
    "\n",
    "df = (pd.read_csv('IndexFundsData.csv'))\n",
    "\n",
    "print(df.head(10))#make sure we got the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Store data in the h5 file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put('SAP', df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Allow this notebook to access your google drive when prompted, because that's where it stores the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import deque\n",
    "from random import sample\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "\n",
    "# conect to google drive so we can store data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Methodology\n",
    "Since we are comparing the effectiveness of data with different dimensionalities we\n",
    "naturally have to train multiple models.  The state variables for each model is shown\n",
    "in the below table.\n",
    "\n",
    "|  Model 0        | Model 1  | Model 2  |  Model 3 |  Model 4 |  Model 5 |\n",
    "|----------------|-------------------------------------|------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|\n",
    "| - 1 Day Return | - 1 Day Return<br>- Previous Action | - 1 Day Return<br>- Previous Action<br>- Price | - 1 Day Return<br>- 2 Day Return<br>- 5 Day Return<br>- 10 Day Return<br>- 21 Day Return<br>- Previous Action<br>- Price | - 1 Day Return<br>- 2 Day Return<br>- 5 Day Return<br>- 10 Day Return<br>- 21 Day Return<br>- Previous Action<br>- Price<br> - 2 Similar Indexes'<br>&nbsp;&nbsp;  - 1 Day Return<br>&nbsp;&nbsp;  - 5 Day Return<br>&nbsp;&nbsp;  - 21 Day Return | - 1 Day Return<br>- 2 Day Return<br>- 5 Day Return<br>- 10 Day Return<br>- 21 Day Return<br>- Previous Action<br>- Price<br> - 2 Similar Indexes'<br>&nbsp;&nbsp;  - 1 Day Return<br>&nbsp;&nbsp;  - 5 Day Return<br>&nbsp;&nbsp;  - 21 Day Return<br> - 2 Unconnected Indexes'<br>&nbsp;&nbsp;  - 1 Day Return<br>&nbsp;&nbsp;  - 5 Day Return<br>&nbsp;&nbsp;  - 21 Day Return |\n",
    "\n",
    "This notebook allows you to specify which state space you want to use, as training all\n",
    "of them at once would take a very long time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "<mark> Select which model you want to run by setting the model variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.638316Z",
     "start_time": "2021-02-25T06:20:29.636097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "#Which model to run(0 - 6, 0 being the simplest, 6 being the most complex)\n",
    "model = 0\n",
    "whenSave = 10\n",
    "stopAfterOne = True\n",
    "printStep = True\n",
    "trading_cost_bps = 0\n",
    "time_cost_bps = 0\n",
    "batch_size = 256\n",
    "max_episodes = 20\n",
    "epsilon_decay_steps = max_episodes/2\n",
    "\n",
    "#Random setup stuff\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "#Use a GPU is we have one\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "#Set up results directory to google drive\n",
    "results_path = \"/content/gdrive/My Drive/\"\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation Enviornment\n",
    "\n",
    "Our environment is a farily simple market trading simulation.  The agent has a choice of 3 actions where\n",
    "\n",
    "A=E{0,1,2}, Sell Short, Flat, Buy Long\n",
    "\n",
    "0 is when the agent Shorts the index fund equal to the amount of capital it has. 1 is where it transfers all\n",
    "it's capital into cash, and closes all short positions. 2 is where it buys as much of the given fund as possible\n",
    "with the capital it has.  This is a very simplistic model because it can not invest only a portion of it's\n",
    "capital, it is forced to invest all or none of it.\n",
    "\n",
    "Each time step the simulation updates the portfolios net asset value(NAV), and performs the agents requested\n",
    "action. The NAV is calculated with the following formula:\n",
    "\n",
    "new_NAV = old_NAV*(1+reward)\n",
    "\n",
    "The reward function is simply the percent change of the NAV.  This reward function is also used to reward\n",
    "the reinforcement learning agent.  To calculate the reward function the simulation uses the following\n",
    "equations:\n",
    "\n",
    "reward = ((Yesterday’s_Action-1) * 1_Day_Return) – Trading_Costs – Daily_Cost\n",
    "\n",
    "Trading_Costs = 0.001 * |Today’s_Action - Yesterday’s_Action|\n",
    "\n",
    "Daily_Cost = .0001\n",
    "\n",
    "Yesterday's action is subtracted by 1 so that it translates the action space to -1, 0, and 1.  That way\n",
    "if we held cash(now equal to 0) the 1_day_return won't affect the NAV.  If we bought the stock(now equal\n",
    "to 1) the percent change of NAV will be directly correlated to the 1_day_return, and if we shorted the\n",
    "percent change would be inversely correlated to 1_day_return.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.688161Z",
     "start_time": "2021-02-25T06:20:29.681742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Trading costs: 0.10% | Time costs: 0.01%'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simulation variables\n",
    "trading_days = 252\n",
    "#trading_cost_bps = 1e-3\n",
    "#time_cost_bps = 1e-4\n",
    "\n",
    "register(\n",
    "    id='trading-v0',\n",
    "    entry_point='trading_env:TradingEnvironment',\n",
    "    max_episode_steps=trading_days\n",
    ")\n",
    "\n",
    "f'Trading costs: {trading_cost_bps:.2%} | Time costs: {time_cost_bps:.2%}'\n",
    "\n",
    "#Initalize environment\n",
    "trading_environment = gym.make('trading-v0', trading_days = trading_days, model = model)\n",
    "trading_environment.env.trading_days = trading_days\n",
    "trading_environment.env.data_source.trading_days = trading_days\n",
    "trading_environment.env.simulator.steps = trading_days\n",
    "\n",
    "trading_environment.env.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.simulator.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.simulator.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.simulator.reinitialize()\n",
    "trading_environment.seed(42)\n",
    "\n",
    "# Get Environment Params\n",
    "state_dim = len(trading_environment.reset())\n",
    "num_actions = trading_environment.action_space.n\n",
    "max_episode_steps = trading_environment.spec.max_episode_steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "## Define Trading Agent(he Neural Network)\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_dim,\n",
    "                 num_actions,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 replay_capacity,\n",
    "                 architecture,\n",
    "                 l2_reg,\n",
    "                 tau,\n",
    "                 batch_size):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.experience = deque([], maxlen=replay_capacity)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.architecture = architecture\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.online_network = self.build_model()\n",
    "        self.target_network = self.build_model(trainable=False)\n",
    "        self.update_target()\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
    "        self.epsilon_history = []\n",
    "\n",
    "        self.total_steps = self.train_steps = 0\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.losses = []\n",
    "        self.idx = tf.range(batch_size)\n",
    "        self.train = True\n",
    "\n",
    "    def build_model(self, trainable=True):\n",
    "        layers = []\n",
    "        n = len(self.architecture)\n",
    "        for i, units in enumerate(self.architecture, 1):\n",
    "            layers.append(Dense(units=units,\n",
    "                                input_dim=self.state_dim if i == 1 else None,\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=l2(self.l2_reg),\n",
    "                                name=f'Dense_{i}',\n",
    "                                trainable=trainable))\n",
    "        layers.append(Dropout(.1))\n",
    "        layers.append(Dense(units=self.num_actions,\n",
    "                            trainable=trainable,\n",
    "                            name='Output'))\n",
    "        model = Sequential(layers)\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_network.set_weights(self.online_network.get_weights())\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps += 1\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        q = self.online_network.predict(state)\n",
    "        return np.argmax(q, axis=1).squeeze()\n",
    "\n",
    "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
    "        if not_done:\n",
    "            self.episode_reward += r\n",
    "            self.episode_length += 1\n",
    "        else:\n",
    "            if self.train:\n",
    "                if self.episodes < self.epsilon_decay_steps:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                else:\n",
    "                    self.epsilon *= self.epsilon_exponential_decay\n",
    "\n",
    "            self.episodes += 1\n",
    "            self.rewards_history.append(self.episode_reward)\n",
    "            self.steps_per_episode.append(self.episode_length)\n",
    "            self.episode_reward, self.episode_length = 0, 0\n",
    "\n",
    "        self.experience.append((s, a, r, s_prime, not_done))\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if self.batch_size > len(self.experience):\n",
    "            return\n",
    "        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n",
    "        states, actions, rewards, next_states, not_done = minibatch\n",
    "\n",
    "        next_q_values = self.online_network.predict_on_batch(next_states)\n",
    "        best_actions = tf.argmax(next_q_values, axis=1)\n",
    "\n",
    "        next_q_values_target = self.target_network.predict_on_batch(next_states)\n",
    "        target_q_values = tf.gather_nd(next_q_values_target,\n",
    "                                       tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n",
    "\n",
    "        targets = rewards + not_done * self.gamma * target_q_values\n",
    "\n",
    "        q_values = self.online_network.predict_on_batch(states)\n",
    "        q_values[[self.idx, actions]] = targets\n",
    "\n",
    "        loss = self.online_network.train_on_batch(x=states, y=q_values)\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.575368Z",
     "start_time": "2021-02-25T06:20:32.565067Z"
    }
   },
   "outputs": [],
   "source": [
    "#RL hypers\n",
    "gamma = .99,  # discount factor\n",
    "tau = 100  # target network update frequency\n",
    "\n",
    "### NN Architecture\n",
    "\n",
    "architecture = (256, 256)  # units per layer\n",
    "learning_rate = 0.0001  # learning rate\n",
    "l2_reg = 1e-6  # L2 regularization\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "replay_capacity = int(1e6)\n",
    "#batch_size = 4096\n",
    "\n",
    "### epsilon-greedy Policy\n",
    "\n",
    "epsilon_start = 1.0 # starting point for epsilon\n",
    "epsilon_end = .01 # ending point for epsilon\n",
    "#epsilon_decay_steps = 250 # the number of steps to get from start to end\n",
    "epsilon_exponential_decay = .99 # after 250 step(epsilon_decay_steps) epsilon = epsilon*epsilon_exponential_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.613239Z",
     "start_time": "2021-02-25T06:20:32.604766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_1 (Dense)             (None, 256)               512       \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,075\n",
      "Trainable params: 67,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#clear out karas\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#instantiate the ddqn model\n",
    "ddqn = DDQNAgent(state_dim=state_dim,\n",
    "                 num_actions=num_actions,\n",
    "                 learning_rate=learning_rate,\n",
    "                 gamma=gamma,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                 replay_capacity=replay_capacity,\n",
    "                 architecture=architecture,\n",
    "                 l2_reg=l2_reg,\n",
    "                 tau=tau,\n",
    "                 batch_size=batch_size)\n",
    "\n",
    "ddqn.online_network.summary()\n",
    "\n",
    "### Set Experiment parameters\n",
    "\n",
    "total_steps = 0\n",
    "#max_episodes = 1000\n",
    "\n",
    "### Initialize Experiment variables\n",
    "\n",
    "episode_time, navs, market_navs, diffs, episode_eps, holds, shorts, buys = [], [], [], [], [], [], [], []\n",
    "test_navs, test_market_navs, test_diffs, test_holds, test_shorts, test_buys = [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.752721Z",
     "start_time": "2021-02-25T06:20:32.742471Z"
    }
   },
   "outputs": [],
   "source": [
    "#prints the results from the training and testing runs\n",
    "def track_results(episode, nav_ma_100, nav_ma_10,\n",
    "                  market_nav_100, market_nav_10,\n",
    "                  win_ratio, total, epsilon, pretext=\"Training Results:\"):\n",
    "    time_ma = np.mean([episode_time[-100:]])\n",
    "    T = np.sum(episode_time)\n",
    "    \n",
    "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
    "    print(pretext + template.format(episode, format_time(total),\n",
    "                          nav_ma_100-1, nav_ma_10-1, \n",
    "                          market_nav_100-1, market_nav_10-1, \n",
    "                          win_ratio, epsilon))\n",
    "\n",
    "#Runs a year long simulation on the testing data\n",
    "def test_data_simulation():\n",
    "    #reset the environment\n",
    "    testthis_state = trading_environment.reset(training=False)\n",
    "    num_holds = 0\n",
    "    num_buys = 0\n",
    "    num_shorts = 0\n",
    "\n",
    "    #loop for a year\n",
    "    for test_episode_step in range(max_episode_steps):\n",
    "        testaction = ddqn.epsilon_greedy_policy(testthis_state.reshape(-1, state_dim))\n",
    "        testnext_state, testreward, testdone, _ = trading_environment.step(testaction)\n",
    "\n",
    "        if testaction == 0:\n",
    "            num_shorts += 1\n",
    "        elif testaction == 1:\n",
    "            num_holds += 1\n",
    "        else:\n",
    "            num_buys += 1\n",
    "\n",
    "        if testdone:\n",
    "            break\n",
    "        testthis_state = testnext_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_holds.append(num_holds)\n",
    "    test_shorts.append(num_shorts)\n",
    "    test_buys.append(num_buys)\n",
    "    # get DataFrame with seqence of actions, returns and nav values\n",
    "    test_result = trading_environment.env.simulator.result()\n",
    "\n",
    "    # get results of last step\n",
    "    test_final = test_result.iloc[-1]\n",
    "\n",
    "\n",
    "    # get nav\n",
    "    test_nav = test_final.nav\n",
    "    test_navs.append(test_nav)\n",
    "\n",
    "    # market nav\n",
    "    test_market_nav = test_final.market_nav\n",
    "    test_market_navs.append(test_market_nav)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    test_diff = test_nav - test_market_nav\n",
    "    test_diffs.append(test_diff)\n",
    "\n",
    "    #Store the results\n",
    "    track_results(episode,\n",
    "                  # show mov. average results for 100 (10) periods\n",
    "                  np.mean(test_navs[-100:]),\n",
    "                  np.mean(test_navs[-10:]),\n",
    "                  np.mean(test_market_navs[-100:]),\n",
    "                  np.mean(test_market_navs[-10:]),\n",
    "                  # share of agent wins, defined as higher ending nav\n",
    "                  np.sum([s > 0 for s in test_diffs[-100:]])/min(len(test_diffs), 100),\n",
    "                  time() - start, -1, pretext=\"Testing Results:\")\n",
    "\n",
    "\n",
    "def saveData():\n",
    "    print(len(diffs))\n",
    "\n",
    "    exampleState = trading_environment.reset()\n",
    "    numStateVars = len(exampleState)\n",
    "\n",
    "    results = pd.DataFrame({'NumStateVars': numStateVars,\n",
    "                            'Episode': list(range(1, episode+1)),\n",
    "                            'TrainAgent': navs,\n",
    "                            'TrainMarket': market_navs,\n",
    "                            'TrainDifference': diffs,\n",
    "                            'Holds': holds,\n",
    "                            'Buys': buys,\n",
    "                            'Shorts': shorts}).set_index('Episode')\n",
    "\n",
    "    results['Strategy Wins (%)'] = (results.TrainDifference > 0).rolling(100).sum()\n",
    "\n",
    "\n",
    "    test_results = pd.DataFrame({'NumStateVars': numStateVars,\n",
    "                            'EpisodeDiv10': list(range(1, len(test_navs)+1)),\n",
    "                            'TestAgent': test_navs,\n",
    "                            'TestMarket': test_market_navs,\n",
    "                            'TestDifference': test_diffs,\n",
    "                            'Holds': test_holds,\n",
    "                            'Buys': test_buys,\n",
    "                            'Shorts': test_shorts}).set_index('EpisodeDiv10')\n",
    "\n",
    "\n",
    "    test_results['Strategy Wins (%)'] = (test_results.TestDifference > 0).rolling(100).sum()\n",
    "\n",
    "\n",
    "    #Get the date and time so we can keep track of the data files\n",
    "    currentTime = datetime.now()\n",
    "    training_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TrainResults.csv'\n",
    "    testing_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TestResults.csv'\n",
    "\n",
    "    #store the results in a csv\n",
    "    results.to_csv(results_path + training_file_name)\n",
    "    test_results.to_csv(results_path + testing_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Results:   1 | 00:00:00 | Agent:  11.5% ( 11.5%) | Market:   8.7% (  8.7%) | Wins: 100.0% | eps: -1.000\n",
      "Training Results:   1 | 00:00:00 | Agent: -24.0% (-24.0%) | Market: -16.9% (-16.9%) | Wins:  0.0% | eps:  0.996\n",
      "Testing Results:   2 | 00:00:02 | Agent:   5.3% (  5.3%) | Market:   5.1% (  5.1%) | Wins: 50.0% | eps: -1.000\n",
      "Training Results:   2 | 00:00:02 | Agent: -22.7% (-22.7%) | Market: -14.3% (-14.3%) | Wins:  0.0% | eps:  0.992\n",
      "Testing Results:   3 | 00:00:02 | Agent:   5.5% (  5.5%) | Market:   2.9% (  2.9%) | Wins: 66.7% | eps: -1.000\n",
      "Training Results:   3 | 00:00:02 | Agent: -21.8% (-21.8%) | Market: -11.6% (-11.6%) | Wins:  0.0% | eps:  0.988\n",
      "Testing Results:   4 | 00:00:03 | Agent:  -3.5% ( -3.5%) | Market:   1.5% (  1.5%) | Wins: 50.0% | eps: -1.000\n",
      "Training Results:   4 | 00:00:03 | Agent: -21.9% (-21.9%) | Market:  -6.6% ( -6.6%) | Wins:  0.0% | eps:  0.984\n",
      "Testing Results:   5 | 00:00:03 | Agent:  -8.4% ( -8.4%) | Market:   0.3% (  0.3%) | Wins: 40.0% | eps: -1.000\n",
      "Training Results:   5 | 00:00:03 | Agent: -22.3% (-22.3%) | Market:  -7.8% ( -7.8%) | Wins:  0.0% | eps:  0.980\n",
      "Testing Results:   6 | 00:00:03 | Agent: -10.9% (-10.9%) | Market:   1.8% (  1.8%) | Wins: 33.3% | eps: -1.000\n",
      "Training Results:   6 | 00:00:03 | Agent: -21.7% (-21.7%) | Market:  -9.4% ( -9.4%) | Wins:  0.0% | eps:  0.976\n",
      "Testing Results:   7 | 00:00:03 | Agent: -14.0% (-14.0%) | Market:   1.6% (  1.6%) | Wins: 28.6% | eps: -1.000\n",
      "Training Results:   7 | 00:00:04 | Agent: -21.9% (-21.9%) | Market:  -8.0% ( -8.0%) | Wins:  0.0% | eps:  0.972\n",
      "Testing Results:   8 | 00:00:04 | Agent: -15.4% (-15.4%) | Market:   0.1% (  0.1%) | Wins: 25.0% | eps: -1.000\n",
      "Training Results:   8 | 00:00:05 | Agent: -20.9% (-20.9%) | Market:  -8.0% ( -8.0%) | Wins:  0.0% | eps:  0.968\n",
      "Testing Results:   9 | 00:00:05 | Agent: -16.4% (-16.4%) | Market:   1.0% (  1.0%) | Wins: 22.2% | eps: -1.000\n",
      "Training Results:   9 | 00:00:05 | Agent: -22.3% (-22.3%) | Market:  -8.5% ( -8.5%) | Wins:  0.0% | eps:  0.964\n",
      "Testing Results:  10 | 00:00:05 | Agent: -15.5% (-15.5%) | Market:   1.5% (  1.5%) | Wins: 20.0% | eps: -1.000\n",
      "Training Results:  10 | 00:00:06 | Agent: -23.0% (-23.0%) | Market:  -9.2% ( -9.2%) | Wins:  0.0% | eps:  0.960\n",
      "Testing Results:  11 | 00:00:07 | Agent: -17.0% (-19.8%) | Market:   1.1% (  0.4%) | Wins: 18.2% | eps: -1.000\n",
      "Training Results:  11 | 00:00:07 | Agent: -25.3% (-25.4%) | Market:  -8.2% ( -7.3%) | Wins:  0.0% | eps:  0.956\n",
      "Testing Results:  12 | 00:00:07 | Agent: -17.0% (-21.5%) | Market:   1.6% (  0.9%) | Wins: 16.7% | eps: -1.000\n",
      "Training Results:  12 | 00:00:08 | Agent: -25.7% (-26.3%) | Market:  -9.2% ( -8.2%) | Wins:  0.0% | eps:  0.952\n",
      "Testing Results:  13 | 00:00:08 | Agent: -17.9% (-24.9%) | Market:   1.6% (  1.2%) | Wins: 15.4% | eps: -1.000\n",
      "Training Results:  13 | 00:00:09 | Agent: -27.2% (-28.8%) | Market:  -7.9% ( -6.8%) | Wins:  0.0% | eps:  0.949\n",
      "Testing Results:  14 | 00:00:09 | Agent: -17.6% (-23.3%) | Market:   1.6% (  1.7%) | Wins: 14.3% | eps: -1.000\n",
      "Training Results:  14 | 00:00:10 | Agent: -26.5% (-28.3%) | Market:  -8.4% ( -9.2%) | Wins:  0.0% | eps:  0.945\n",
      "Testing Results:  15 | 00:00:11 | Agent: -16.8% (-21.0%) | Market:   2.1% (  2.9%) | Wins: 13.3% | eps: -1.000\n",
      "Training Results:  15 | 00:00:11 | Agent: -25.5% (-27.2%) | Market:  -9.3% (-10.1%) | Wins:  6.7% | eps:  0.941\n",
      "Testing Results:  16 | 00:00:12 | Agent: -16.5% (-19.8%) | Market:   2.0% (  2.1%) | Wins: 12.5% | eps: -1.000\n",
      "Training Results:  16 | 00:00:12 | Agent: -24.9% (-26.9%) | Market:  -8.8% ( -8.4%) | Wins:  6.2% | eps:  0.937\n",
      "Testing Results:  17 | 00:00:13 | Agent: -16.7% (-18.6%) | Market:   1.6% (  1.6%) | Wins: 11.8% | eps: -1.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6084/392907737.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m                                  0.0 if done else 1.0)\n\u001B[0;32m     15\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mddqn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m             \u001B[0mddqn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperience_replay\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m             \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"model: \", model)\n",
    "print(\"whenSave: \", whenSave)\n",
    "print(\"trading_cost_bps: \", trading_cost_bps)\n",
    "print(\"time_cost_bps: \", time_cost_bps)\n",
    "print(\"batch_size: \", batch_size)\n",
    "print(\"max_episodes: \", max_episodes)\n",
    "print(\"epsilon_decay_steps: \", epsilon_decay_steps)\n",
    "print(\"-----------------------------------------------\")\n",
    "for episode in range(1, max_episodes + 1):\n",
    "    this_state = trading_environment.reset()\n",
    "    numBuy = 0\n",
    "    numShort = 0\n",
    "    numHold = 0\n",
    "    print(\"Episode: \", episode)\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "\n",
    "        if action == 0:\n",
    "            numShort += 1\n",
    "        elif action == 1:\n",
    "            numHold += 1\n",
    "        else:\n",
    "            numBuy += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        next_state, reward, done, _ = trading_environment.step(action)\n",
    "\n",
    "        ddqn.memorize_transition(this_state, \n",
    "                                 action, \n",
    "                                 reward, \n",
    "                                 next_state, \n",
    "                                 0.0 if done else 1.0)\n",
    "        if printStep:\n",
    "            print(\"Step: \", episode_step)\n",
    "            print(\"ts: \", this_state)\n",
    "            print(\"ac: \", action)\n",
    "            print(\"re: \", reward)\n",
    "            print(\"ns: \", next_state)\n",
    "\n",
    "        if ddqn.train:\n",
    "            ddqn.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "        this_state = next_state\n",
    "\n",
    "    if stopAfterOne:\n",
    "        break\n",
    "\n",
    "    # get DataFrame with seqence of actions, returns and nav values\n",
    "    result = trading_environment.env.simulator.result()\n",
    "    \n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # get nav\n",
    "    nav = final.nav\n",
    "    navs.append(nav)\n",
    "\n",
    "    # market nav \n",
    "    market_nav = final.market_nav\n",
    "    market_navs.append(market_nav)\n",
    "\n",
    "    #num holds buys and sells\n",
    "    holds.append(numHold)\n",
    "    buys.append(numBuy)\n",
    "    shorts.append(numShort)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    diff = nav - market_nav\n",
    "    diffs.append(diff)\n",
    "    if episode % 10 == 0:\n",
    "        track_results(episode, \n",
    "                      # show mov. average results for 100 (10) periods\n",
    "                      np.mean(navs[-100:]), \n",
    "                      np.mean(navs[-10:]), \n",
    "                      np.mean(market_navs[-100:]), \n",
    "                      np.mean(market_navs[-10:]), \n",
    "                      # share of agent wins, defined as higher ending nav\n",
    "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100), \n",
    "                      time() - start, ddqn.epsilon)\n",
    "        test_data_simulation()\n",
    "    if episode % whenSave == 0:\n",
    "        saveData()\n",
    "    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n",
    "        print(result.tail())\n",
    "        break\n",
    "\n",
    "if not stopAfterOne:\n",
    "    print(\"final\")\n",
    "    track_results(episode,\n",
    "                  # show mov. average results for 100 (10) periods\n",
    "                  np.mean(navs[-100:]),\n",
    "                  np.mean(navs[-10:]),\n",
    "                  np.mean(market_navs[-100:]),\n",
    "                  np.mean(market_navs[-10:]),\n",
    "                  # share of agent wins, defined as higher ending nav\n",
    "                  np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
    "                  time() - start, ddqn.epsilon)\n",
    "    test_data_simulation()\n",
    "    trading_environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "saveData()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    sns.distplot(results.TrainDifference)\n",
    "    sns.despine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.031Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1008x288 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAEYCAYAAADPrtzUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA5TklEQVR4nO3deUBU9d7H8c8AiiLuWmmakive3L2lKeFS7uCKO2l6NXMpzCxwT83tUlbuttyMzA20XMpMTTPDJU0rxX3D+5goKomsMr/nDx/nCQXBcuSI79dfzJmzfM93hvnNZ+acMzZjjBEAAAAAAMhRLjldAAAAAAAAIKADAAAAAGAJBHQAAAAAACyAgA4AAAAAgAUQ0AEAAAAAsAACOgAAAAAAFkBAR66WmpqqRo0aqV+/fjmy/eDgYH300Ue3TF+xYoXq1q2rdu3aqV27dvL391fTpk31+uuvKzk5Ocv1jh49Wr/99pszSs7Sli1bNGPGDElS06ZNVatWLV29ejXdPCtXrlSVKlW0bt26v7SNjRs3atKkSX+71j8LCwtTlSpVtHfv3ru63rvt6tWr+te//qWkpKScLgVALnevxsi+ffvq4sWLGd6XkJCgadOmqUWLFvLz85Ofn59mzJiRrdfAK1eu6Pnnn7+rtZ49e1Zt27aVv7+/fv7557u67pwWGhqqrVu3SpJCQkLk7++vQYMGKTU1VZIUFxenrl27KiUlJVvrmzlzpqpUqaLw8PB00xMSElS7dm29+OKLf7nWdu3a6Y8//vjLy9/s4sWLqlGjhsaOHXvX1uksU6dO1Y4dO3K6DOQgAjpytW+//VZVqlTR/v37dezYsZwuJ5169erpyy+/1JdffqlVq1bp66+/1tGjR7Vy5cosl/3xxx9ljLkHVaYXHx+v0NBQDRw40DGtaNGi+vbbb9PNt3LlSpUoUeIvb6dZs2YaPXr0X14+I0uWLJGfn58WLlx4V9d7txUoUEBt27bVe++9l9OlAMjl7tUYuW3btgynX7t2TS+88ILsdru++OILrV69WsuWLdPVq1fVr18/Xbt27bbrjYuL06+//npXa92xY4dKlCihVatWqXbt2nd13Tlp7969Onr0qHx8fHTw4EHFxMRo1apVKlGihH744QdJ0nvvvaeBAwcqb9682V5v6dKltWrVqnTT1q9fLw8Pj79V75dffqlChQr9rXX8WUREhJo1a6a1a9fq8uXLd229zjB48GBNmjSJD+ofYG45XQDgTIsXL1br1q1Vrlw5LVy4UBMmTNCOHTs0Y8YMlS1bVkeOHFFKSorGjh2r+vXrKzg4WJ6enjp06JB+//13Pf7443rnnXdUoEABValSRZGRkSpWrJgkOW4XKVJEkydP1r59+3T16lUZYzRp0iTVrVv3jmq9fPmy4uPjVbhwYUnSuXPnNGHCBJ09e1apqalq06aNBg4cqBkzZigmJkavvfaapk+frtDQUPXs2VMtW7aUJAUGBjpuP/HEE2rWrJkOHjyo0NBQ9ejRQwMGDNC2bdsUExOj559/Xn369NH58+f1xhtv6NKlS5IkX19fBQUF3VLj559/rkaNGil//vyOaf7+/lq1apXat28vSfrvf/+rhIQEPf744455fvrpJ02fPl2JiYnKkyePgoKC9Mwzz6hbt27q06ePo/bQ0FAZY1ShQgV98803mj9/vgIDA1WrVi3t2bNHZ8+eVd26dTVt2jS5uLhoxYoVWrBggfLly6f69evr008/1YEDB26pe8eOHYqLi9OIESP03HPP6ezZsypVqpSWLl2qTZs2af78+ZKkY8eOqU+fPtq8ebNOnjypt956S5cvX1ZaWpoCAwPVuXNn7dixQ2+99ZY8PDyUkJCg8PBwTZ8+PcPH/+LFiwoJCdHp06dVpEgRlSxZUpUqVdLQoUN17NixDNcvSa1atVJoaKj69ev3tz7oAIDbyWiMlKQFCxYoPDxcBQoUUL169bRx40Zt2rRJKSkpCg0N1a5du5SWlqZq1app9OjR8vT0VNOmTdWhQwdFRkbq7NmzatWqlV5//XWFhIRIknr37q0FCxaoVKlSju2vW7dOdrvdMY8k5c+fX6NGjVL79u317bffqnr16vLz83N8m33mzBnH7ZCQECUlJaldu3ZasWKFqlevrt69e2vHjh1KSEjQq6++qubNm0uSli9frsWLF8tut6tIkSIaM2aMKlSooODgYF2+fFnR0dHy8PDQ+fPndeXKFQUGBiosLExLly5VWFiYXFxcVKJECY0ZM0ZeXl7plmvcuLFiY2Pl7u6uX3/9VRcuXFCrVq1UrFgxfffddzp//rwmTZqkBg0a6MSJE5owYYISEhIUExOjqlWr6t1335W7u7uqV6+e4RgtSfPnz9fKlSvl5uamcuXKaerUqSpYsGCm+3WzmTNnqlevXpKkvHnzKiUlRcYYx7h88OBBnT17Vk2aNLmj55CPj482bNig33//XY888oik6x/S+/v76/jx45KuH+nw5ptv6uDBg7LZbPLx8dGrr76qiIiITMfgatWqKTIyUps3b9a3334rFxcXnTp1Snny5NG0adNUuXJlnTp1SiNHjlRcXJxKliwpY4z8/f3VsWPHdDXa7XYtXbpUY8eOVUJCgpYuXaoXX3xRV65cka+vr7755huVLFlSktSlSxcNHjxYDRo0uO1zvUaNGjp06JBeffVVubm5af78+UpJSdHFixfVvn17x3uov/K/VLBgQdWuXVtLly5V79697+jxQC5hgFzqyJEj5oknnjCXLl0y+/btMzVq1DAXL14027dvN97e3ubAgQPGGGM++ugj07NnT2OMMW+88Ybp2rWrSU5ONikpKaZ9+/YmPDzcGGNM5cqVTWxsrGP9N27v2bPHDB061KSlpRljjJk/f7558cUXHev78MMPb6ktIiLC1KlTx/j7+5uWLVuap556ynTt2tUsXrzYMU9gYKDZuHGjMcaYpKQkExgYaNauXWuMMaZJkybml19+McYY06tXL/P11187lvvz7cqVK5uVK1emqzksLMwYY8yvv/5qnnjiCZOUlGRmzZplxowZY4wx5urVqyYoKMj88ccft9TdoUMHs337dsftJk2amN27d5v69eubc+fOGWOMmT17tgkLC3PUcfHiRdOgQQOzd+9eY4wxhw8fNk8++aQ5ffq0CQ8PNwMGDDDGGHPt2jXj4+NjTpw4YSIiIhzTe/XqZV5++WWTlpZmrly5Yho1amQiIyPNkSNHTIMGDczZs2eNMcbMnDnTVK5c+ZaajTHmlVdeMVOnTjXGGNO/f38zffp0Y4wxV65cMXXr1jUxMTHGGGOmT59u3nnnHZOammpat25tfvvtN2OMMX/88Ydp1aqV+fnnn8327dtN1apVzZkzZ4wx5raP/7BhwxzbOnfunGnYsKF5//33b7v+G4YOHep47gHA3ZbZGPn999+bFi1amLi4OGO3201ISIhp0qSJMeb66+zUqVON3W43xhjz9ttvm3Hjxhljro8HN15nf//9d1O9enVz+vRpY8yt4+cNEyZMcCxzsylTppiJEyea6OhoU6tWLcf0P9+++b7KlSubuXPnGmOMiYqKMnXr1jWxsbFmx44dpkePHiYhIcEYY8zWrVtNq1atjDHXx+nevXs71vHn8efHH380zz77rKP2iIgI06pVK2O3229Z7o033jABAQEmJSXFxMTEmMqVK5tPP/3UGGPMJ598Yl544QVjjDFTp041X3zxhTHGmJSUFNO2bVuzbt06R/0ZjdEbNmwwzZs3N5cvXzbGGDN58mQzZ86c2+7Xn8XFxZmaNWua5ORkx7R33nnH+Pv7mzFjxpi0tDTTp08fc/LkyQwfi8y8//775s033zQTJkww8+fPN8YY89///td06tQpXR9ff/11M3HiRGO3201ycrLp27evmT9/fqZj8I1exMbGmoiICFO3bl3HWD9hwgTz+uuvG2OM6dKli1m0aJExxpijR4+amjVrmoiIiFvq3Lx5s3n66adNamqq+eqrr4yPj49JSUlx1HbjfdrRo0dN48aNTVpaWpbP9VmzZhljjLHb7aZXr17mxIkTxpjrz31vb28TGxv7l/+XjDHmu+++c7w3xYOHb9CRay1evFiNGzdWkSJFVKRIEZUpU0ZLly5V7dq1Vbp0aXl7e0uSqlWrlu6wch8fH8fhXZUrV1ZcXNxtt1O7dm0VLlxYS5YsUXR0tHbs2KECBQpkWV+9evU0f/582e12zZkzR6tXr1azZs0kXT9/a9euXYqLi3Mc6pyQkKCDBw+qdevWd9SHevXqpbt9Yxv/+Mc/lJKSooSEBPn4+GjAgAE6e/asnn76aQ0fPlwFCxa8ZV0nTpxQuXLl0k3LkyePWrZsqTVr1qhv37766quv9Nlnn+mbb76RJP3yyy967LHHVLNmTUlSpUqVVKdOHe3cuVOtWrXS9OnTdf78eR04cEDlypVT+fLltWfPnnTbaNKkiVxcXOTp6aly5copLi5OBw8eVMOGDR2f2Pfq1UszZ868pebz589rw4YNioiIkCS1b99e48eP1+DBg+Xp6akWLVpo1apV6tOnj1atWqXPP/9cJ0+e1OnTpzVy5EjHepKSknTgwAFVqFBBpUqV0qOPPirp9o//li1bHM+thx56yHGkwO3WX6tWLUnSY489phMnTmT8oALA35TZGHnhwgW1bNnScXhxz549tX37dknS5s2bdeXKFf3444+Srp/DXrx4ccc6b4wvDz/8sIoXL664uDiVLVv2tnVkdhh7SkqKXF1d73i/bnxLXLVqVVWuXFm7du3Svn37dOrUKXXr1s0xX1xcnONQ58yOeNu6datat27tOHKuY8eOeuutt3TmzJkMl2vSpIny5MmjkiVLysPDQz4+PpKuv57f2NaIESO0bds2ffDBBzp58qRiYmKUkJDgWEdGY3RkZKRatmzpOMLuxhEH06dPz3S/ihQp4ph26tQplSxZMt2h68OGDdOwYcMkXT+cvHr16vL09NSwYcOUmJiowMBANWzYMKt2S7p+vvioUaM0YMAAffnll44j6m74/vvvtXjxYtlsNuXNm1fdunXTwoULNWDAgAzH4Jv94x//cIz11apV07fffqu4uDj98ssv+uyzzyRJFSpUUP369TOsb/HixfLz85Obm5uaNWumcePGad26dfLz81NAQIDefPNN9evXTxEREerYsaNcXFyyfK7feG9ls9k0b948bd68WWvWrNGxY8ccRyZs2bLlL/8vlS1blvcADzACOnKlhIQEffHFF3J3d1fTpk0lXT9/etGiRapevbry5cvnmNdms6U7n/t2993w5wuobN68WW+99ZZeeOEFNWvWTI8//vgt52PdjouLi4YMGaKff/5Zo0aN0oIFC2S322WM0ZIlSxyHk1+8eFHu7u4ZruPPNd642MsNN58HdmMdNpvNsWyNGjW0ceNGRUZGavv27QoICNDs2bNVp06ddMvabDalpaXdsv327dtr3LhxqlWrlh5//PF0bwzsdnuG9V67dk0eHh5q0aKF1qxZo59//lkBAQEZ7l9Gj4mrq2u6/c7sjdzy5cslSS+99JKjnvj4eK1cuVI9e/ZUQECA45DAihUrqmzZsjp06JAKFSqkL7/80rGeCxcuqGDBgtq7d2+6nt7u8Xdzc0tXo4vL9ct+pKWlZbr+G9LS0u7oPEAAyK7bjZFt2rTJ9LXVbrdr5MiR8vX1lXT9opZ/vrDpn8eozMbPP6tTp44+/PBD2e12x+vjje3s2rVLL7300i3ruXmMu9nN9bq6usput6tdu3YaMWKEY3pMTIwj8GZ2vnRG9d8YvzJa7ubXbDe3W99mv/rqq0pLS1OrVq3UuHFjnT17Nt12MhqjXV1dHbcl6Y8//tAff/yR5X7d4OLikuHYLV1/3D/77DMtXLhQ8+bNk6+vr1q2bKlOnTpp7dq1GS5zsxo1aigtLU1RUVH66quvFBYWpk2bNjnuv/l9gN1ud/QwozH4Zpm9B7jRnxsyeh/w3//+V1u2bNH+/fu1fv16Sdc/FFq4cKH8/PxUr149Xbt2Tb/88ovWrFmjJUuWOGq83XP9xmOfkJCgDh066Nlnn1W9evXUqVMnbdiwQcaYW94D3Mn/0s3/E3iw8MgjV1q9erWKFi2qrVu3atOmTdq0aZM2bNighIQExcbG/qV1FitWzHExmj9fFG3btm1q0qSJevTooerVq2vDhg2ZDoS3M27cOEVGRmrDhg3y9PRUrVq19J///EfS9cG4e/fu2rhxo6TrL/I3BrdixYo5ruh++vRpHTp06I63HRoaqjlz5ujZZ5/VqFGjVLFiRZ08efKW+cqXL6/o6OhbptesWVNJSUmaMWOGOnTocMt9J06c0C+//CJJOnLkiHbt2qUnn3xS0vXzvVasWKGff/5ZLVq0yHbNjRo1UmRkpM6dOyfp/4P4n6WlpWnZsmV68803Hc+DzZs368UXX9Snn34qY4zjG+vZs2c7PiDw8vKSu7u7I0DfuKpvRlfOv93j7+vr67i67aVLl7RhwwbZbLZsrf/MmTPy8vLKdj8AILtuN0ZWq1ZN69ev15UrVyQp3RW6GzVqpEWLFiklJUV2u11jxozRO++8k+X2/jxm/VmLFi2UP39+TZ482XFBrKSkJE2cOFEFChTQc889p0KFCik1NVVHjx6VlH78dXNzU1paWroQ9MUXX0iS9u/frxMnTuif//ynGjZsqLVr1yomJkbS9W9Us3Nub6NGjfTVV185rkAfERGhIkWK3HIk2Z344YcfNHjwYLVu3Vo2m0379u3L8j3D008/rW+//Vbx8fGSrp9P/sknn2R7v8qWLauLFy9m+Csxs2fP1gsvvCAPDw+lpKQoT548cnFxUWJi4h3tV7t27TR58mR5eXml+5Be+v/njTFGKSkpWrZsmZ5++mlJynAMzg5PT0/VqVNHK1askCRFR0crMjIy3QcZkrR06VLVrVs33XN9xYoVOnDggHbv3i3p+ocEEydOVJUqVVS6dOl0NWf1XD916pTi4+MVFBSkpk2baufOnY5lfH19//L/UnR0dLpr+eDBQkBHrrR48WK98MIL6T6tLFSokAIDA//yVbxHjx6tCRMmqEOHDjpw4IDjgiLdunXTrl275Ofnp65du6ps2bI6c+ZMht8c385jjz2m/v37a8qUKUpOTlZoaKj27dvnOATrxs++SNKzzz6rYcOG6YcfftBLL72kbdu2qW3btgoNDb3lkPbs6N27tw4ePKi2bduqU6dOKlOmjNq2bXvLfC1btnT8RMvN2rVrpxMnTjgO6buhWLFieu+99zRx4kT5+flp+PDhmjJliiN8PvHEE3Jzc1OLFi0yPUIgI15eXgoJCVG/fv3UsWNHHTt2LN3F6yTpu+++k91ul5+fX7rpffr00YULF7RlyxZJ1wfn6OhoPfvss5KufwsyZ84chYeHy8/PT3379tUrr7yS4WGQt3v8Q0JCdPz4cfn5+enll19W6dKllS9fvizXn5KSop9//tnxzRYA3E1ZjZFdunRR165d1bFjR125csXx2jpo0CA9+uij6tChg1q3bi1jjIKDg7Pc3nPPPacePXro8OHD6aa7ubnp448/loeHhzp27Ki2bduqQ4cO8vDw0Mcff6w8efKoYMGCGjFihPr3769OnTqlC2AlS5ZUtWrV1KpVK8dFTvfs2aMOHTpo5MiRmjFjhgoXLiwfHx/1799fffv2lZ+fn9asWaNZs2bdEuZu1rBhQ/Xp00e9e/dWmzZt9MUXX2j+/Pl/65vNYcOGafDgwerYsaPGjRunf/7znzp9+vRtl/H19VXHjh3VvXt3+fn56cKFCwoKCsr2fhUqVEh169Z1HF59w7Fjx3T48GHHqXNdu3bVRx99pI4dOzqOOjt37pzatWvn+DA8M/7+/vrpp59u+ZBeuv7+6eLFi46f0fPy8kr3azA3j8HZNW3aNH399dfy9/fXhAkTVKZMmXTftqekpCg8PFz/+te/0i1Xvnx5tWnTxvF+sH379oqKikr3AUF2n+tVqlRR48aN1apVK3Xo0EGbNm1SxYoVderUKTVo0OAv/y9t3brVcVocHjw2k9XxRwDwf+Lj49WlSxdFRETcEobvtejoaH355ZcaNGiQXFxctH79en3wwQcZfpOeUxYtWqRq1aqpdu3aSklJUY8ePTR06FDHIW2ZWbFihY4cOaI33njjHlUKANf9+uuv+vnnnx2/L/6f//xH+/bt07vvvpuzhWXDzb+2gv+3Z88ezZs3TwsWLLjjZUeMGKGRI0eqaNGiTqjsr5s7d66aN2+uChUq6MqVK/L399cHH3ygihUr5nRpkv76/9KVK1fUvXt3RURE3NEXF8g9OAcdQLZ5enrq1Vdf1Zw5czR8+PAcreWRRx5RTEyM/Pz85OrqqoIFC2ry5Mk5WtPNKlasqIkTJ8putys1NVUtW7bMMpzHx8c7vgUBgHvNy8tLH3zwgZYtWyabzaZSpUpp4sSJOV0W/qY6derIy8tL33//vZ555plsL5eYmKhGjRpZLpxL178JHzZsmOMc+/79+1smnEt//X9p1qxZGjlyJOH8AcY36AAAAAAAWADnoAMAAAAAYAEEdAAAAAAALMAyAf3IkSM5XUKOy+hnrfD30VfnoK/OQ2+dg75mH2Myzxdnoa/OQV+dh946B33NnGUCeka/j/mgudPfnET20FfnoK/OQ2+dg75mH2Myzxdnoa/OQV+dh946B33NnGUCOgAAAAAADzICOgAAAAAAFkBABwAAAADAAtxyugAAwIMlNTVVZ86cUVJS0j3fblRU1D3dZk7Jly+fypQpozx58uR0KQAA4A4Q0AEA99SZM2dUsGBBlS9fXjab7Z5tNzExUfnz579n28spxhjFxsbqzJkz8vLyyulyAADAHeAQdwDAPZWUlKTixYvf03D+ILHZbCpevPg9P0IBAAD8fQR0AMA9Rzh3LvoLAMD9iYAOAAAAAIAFcA46AOCBMnXqVO3fv1/nz59XUlKSypYtq6JFi+r999+/7XILFixQ/fr1VaNGjdvO165dO9WpU0fjxo27azUnJydr1apVCggIuGvrBAAA1kNABwA8UIKDgyVJK1as0PHjx/Xaa69la7kBAwZkOc/u3btVuXJlbd++XfHx8fL09Pxbtd5w/vx5LV++nIAOAEAuR0AHADzwgoODdfnyZV2+fFlz585VaGiofv/9d8XExKhp06YaNmyYgoOD1bp1a124cEFbtmxRUlKSTp8+rf79+6tjx46SpOXLl6tFixYqVaqUvvjiC/Xq1UuSNHv2bG3YsEHFihVTYmKiXnnlFVWrVk2jRo3SpUuXJEmjR49WlSpV1Lx5c9WpU0cnTpxQ8eLFNXPmTM2bN09Hjx7VrFmzNGTIkBzrEwAAcC4COgAgRzWfsUWHz8XftfVVfthT64f53vFy9evXV58+fXTmzBnVqlVLAQEBSk5O1jPPPKNhw4almzc+Pl4fffSRTp48qYEDB6pjx46Kj4/X7t27NWnSJFWsWFGDBw9Wr169dPDgQW3dulXh4eFKTU2Vn5+fJGnevHmqX7++evTooZMnTyokJESLFy9WdHS0Fi5cqFKlSqlbt2769ddfNXDgQB0+fJhwDgBALkdABwDkqL8Spp3hxm+GFylSRL/++qu2b98uT09PpaSk3DJv1apVJUmlSpVy3L9q1SrZ7Xa9+OKLkq4flh4ZGamLFy+qevXqcnV1laurq5544glJ0uHDh7V9+3Z9/fXXkqS4uDhJUtGiRVWqVCnH+pOTk5241wAAwEoI6AAA6P9/mmzFihUqWLCgJkyYoFOnTmnZsmUyxmQ475+Fh4dr3rx5qlSpkqTrgX3RokUaOnSowsLCZLfbde3aNR04cECS9Pjjj8vf319+fn6KjY3V8uXLM123i4uL7Hb7Xd1fAABgPfzMGgAAf9KgQQNt3bpVPXv21Pjx41WuXDnFxMTcdpn9+/fLGOMI55LUokUL7d69W4UKFZKvr6+6dOmiwYMHK0+ePHJzc9PAgQP19ddfKzAwUP/617/SLXuz4sWLKzU1Vf/+97/v2n4CAADr4Rt0AMAD6caF3aTrP712Q6VKlbRq1apb5v/zPDe4u7tr06ZNkqSVK1fecl9kZKRiY2NVqFAhhYeHKyUlRW3atFGpUqVUtGhRzZkz55Z1btu2zfH3jBkzHH9/+eWXd7B3AADgfkRABwDAiYoWLarffvtNnTp1ks1mU0BAgEqXLp3TZQEAAAsioAMA4EQuLi6aMmVKTpcBAADuA5yDDgAAAACABRDQAQAAAACwAAI6AAAAAAAWQEAHAAAAAMACCOgAgAfK1KlTFRgYqJYtW6px48YKDAzUyy+/nOVyCxYs0C+//JLp/cHBwapXr55SUlIc0/bv368qVapox44d2aptxYoVCg0Nzda8u3bt0sGDB7M1LwAAuD9wFXcAwAMlODhY0vUwfPz4cb322mvZWm7AgAFZzlOyZEl9//33evbZZyVJq1evVtmyZf96sbcRERGh1q1bq2rVqk5ZPwAAuPcI6ACAB15wcLAuX76sy5cva+7cuQoNDdXvv/+umJgYNW3aVMOGDVNwcLBat26tCxcuaMuWLUpKStLp06fVv39/dezYUZLUpk0brVmzRs8++6zsdrv279+v6tWrS5Li4+M1atQoXblyRTExMerRo4d69OihwMBAFStWTHFxcWrTpo0k6eLFixo0aJBeeeUV1atXT+PGjdOpU6dkt9sVFBSkAgUKaOvWrdq/f78qVqzI76oDAJBLENABADlrdn3pfNTdW19Jb2nw9jterH79+urTp4/OnDmjWrVqKSAgQMnJyXrmmWc0bNiwdPPGx8fro48+0smTJzVw4EBHQK9Ro4bWr1+vhIQE7d27V0899ZSOHTsmSTp16pTatGmj5s2b69y5cwoMDFSPHj0kSW3bttVzzz2nFStWKDY2Vi+99JJGjhypmjVr6vPPP1fRokU1efJkXbp0Sb169dLatWvl4+Oj1q1bE84BAMhFshXQO3ToIE9PT0lSmTJl5O/vr3fffVdubm4qXry4pk2bJnd3dw0ZMkTnz59XUFCQGjZsqOjoaC1cuFCjR4926k4AAO5jfyFMO4OXl5ckqUiRIvr111+1fft2eXp6pjun/IYbh5WXKlXqlvubNWumjRs36scff9SgQYP0zjvvSJJKlCihhQsXav369fL09NS1a9du2bYkbd26VSVLlpTdbpckHT58WLt373ac/37t2jVdvHjxLu45AACwiiwDenJysowxCgsLc0xr0aKFFi1apBIlSujtt9/W8uXLVbduXT366KOaMmWKgoOD1bBhQ82ZM0fDhw936g4AAHA32Gw2SdfPTS9YsKAmTJigU6dOadmyZTLGZDhvRtq2bavJkyfLZrOlO//8448/Vq1atdSjRw9t375dW7ZsyXB97du3V7t27RQUFKTly5fr8ccf1yOPPKKBAwcqKSlJc+fOVZEiRWSz2W6pCwAA3N+yDOgHDx5UYmKi+vbtq2vXrunVV19VWFiYSpQoIen6J/nu7u7y8PBQcnKykpKS5OHhod27d6t8+fKO+QAAuB80aNBAw4cP1969e5U3b16VK1dOMTEx2V6+QoUKunTpkjp16pRuepMmTTRp0iR99dVXKliwoFxdXTP8dl6SKlWqJH9/f02ZMkVjxozR6NGj1atXL8XHx6tHjx5ycXFRzZo1FRoaqjJlyqhChQp/a58BAIA12EwWH78fOnRI+/btU0BAgE6ePKn+/ftr3bp1cnNz0/r16zVv3jwtXrxY7u7umj17to4fP65Bgwbpvffe04gRI/Thhx+qcOHCCgoKkotL5r/qtnfvXrm7u9/1HbyfJCUlKV++fDldRq5DX52DvjpPbu9tamqqKlWqdM+3a4y57Tffuc2RI0eUJ0+edNO8vb2ztSxjcu7/P8wp9NU56Kvz0FvnoK+Zj8lZBvSUlBTZ7XZHAzt37qyZM2fqm2++0bp16zRnzhwVK1Ys3TKrV6+W3W7X0aNH1bx5c+3cuVNVq1ZVw4YNM91OVFRUtt845Fb0wDnoq3PQV+fJ7b3Nqf1LTExU/vz57/l2c8rf6XNufw5mBz1wDvrqHPTVeeitc9DXzGX+lfb/CQ8P19SpUyVJ586dU3x8vCIiIvTTTz/pk08+uSWcJycna/369fL391diYqJcXV1ls9mUkJDgnD0AAAAAACAXyPIc9M6dOyskJETdu3eXzWbThAkT1LdvX1WrVk39+/eXJLVq1crxUzELFy5UYGCgbDabOnXqpLFjx8rT01OzZ8927p4AAO4bD9rh5vcaF48DAOD+lGVAz5s3r95+++1003777bdM5x8wYIDjb29vby1fvvxvlAcAyG3y5cun2NhYFS9enJDuBMYYxcbGPvDn9gEAcD/K1u+gAwBwt5QpU0ZnzpzR+fPn7+l2U1NTb7loWm6VL18+lSlTJqfLAAAAd4iADgC4p/LkySMvL697vl0uSAMAAKwuy4vEAQAAAAAA5yOgAwAAAABgAQR0AAAAAAAsgIAOAAAAAIAFENABAAAAALAAAjoAAAAAABZAQAcAAAAAwAII6AAAAAAAWAABHQAAAAAACyCgAwAAAABgAQR0AAAAAAAsgIAOAAAAAIAFENABAAAAALAAAjoAAAAAABZAQAcAAAAAwAII6AAAAAAAWAABHQAAAAAACyCgAwAAAABgAQR0AAAAAAAsgIAOAAAAAIAFENABAAAAALAAAjoAAAAAABZAQAcAAAAAwAII6AAAAAAAWAABHQAAAAAACyCgAwAAAABgAQR0AAAAAAAsgIAOAAAAAIAFENABAAAAALAAAjoAAAAAABZAQAcAAAAAwAKyFdA7dOigwMBABQYGKiQkRJKUlpaml19+Wd9//70kyW63a9CgQQoICNC2bdskSdHR0Zo0aZKTSgcAAAAAIPdwy2qG5ORkGWMUFhbmmHb69Gm9/vrrOnfunDp37ixJioqK0qOPPqopU6YoODhYDRs21Jw5czR8+HDnVQ8AAAAAQC6R5TfoBw8eVGJiovr27avnn39ee/fuVUJCgt566y099dRTjvk8PDyUnJyspKQkeXh4aPfu3SpfvrxKlCjh1B0AAAAAACA3sBljzO1mOHTokPbt26eAgACdPHlS/fv317p16+Tm5qbg4GC1bt1azzzzjCRp9uzZOn78uAYNGqT33ntPI0aM0IcffqjChQsrKChILi6Zfx6wd+9eubu73929u88kJSUpX758OV1GrkNfnYO+Og+9dQ76Knl7e2drPsZkni/OQl+dg746D711Dvqa+Zic5SHuXl5eKleunGw2m7y8vFSkSBGdP39epUqVumXewYMHS5JWr16tZs2aadmyZercubN27typyMhINWzYMNPtuLu7Z/uNQ24VFRX1wPfAGeirc9BX56G3zkFfs48xmeeLs9BX56CvzkNvnYO+Zi7LQ9zDw8M1depUSdK5c+cUHx+vkiVLZjp/cnKy1q9fL39/fyUmJsrV1VU2m00JCQl3r2oAAAAAAHKZLL9B79y5s0JCQtS9e3fZbDZNnjxZbm6ZL7Zw4UIFBgbKZrOpU6dOGjt2rDw9PTV79uy7WjgAAAAAALlJlgE9b968evvttzO878Y36382YMAAx9/e3t5avnz53ygPAAAAAIAHQ7Z+Bx0AAAAAADgXAR0AAAAAAAsgoAMAAAAAYAEEdAAAAAAALICADgAAAACABRDQAQAAAACwAAI6AAAAAAAWQEAHAAAAAMACCOgAAAAAAFgAAR0AAAAAAAsgoAMAAAAAYAEEdAAAAAAALICADgAAAACABRDQAQAAAACwAAI6AAAAAAAWQEAHAAAAAMACCOgAAAAAAFgAAR0AAAAAAAsgoAMAAAAAYAEEdAAAAAAALICADgAAAACABRDQAQAAAACwAAI6AAAAAAAWQEAHAAAAAMACCOgAAAAAAFgAAR0AAAAAAAsgoAMAAAAAYAEEdAAAAAAALICADgAAAACABRDQAQAAAACwAAI6AAAAAAAWQEAHAAAAAMAC3LIzU4cOHeTp6SlJKlOmjLp27aq33npLrq6uatSokYYMGaKrV6/qpZdeUnJyst58801VrVpVP/30k/bs2aMBAwY4dScAAAAAALjfZRnQk5OTZYxRWFiYY1q7du00c+ZMlS1bVgMGDNCBAwd05swZNW3aVE8++aTCw8M1atQoffrpp/r3v//t1B0AAAAAACA3yDKgHzx4UImJierbt6+uXbumoUOHKiUlRY899pgkqVGjRvrxxx9VtWpVJScnKykpSR4eHlq9erWee+45ubu7O30nAAAAAAC432UZ0PPly6d+/fopICBAJ0+eVP/+/VWoUCHH/QUKFFB0dLSefvppbdmyRUuXLtXQoUM1ffp0DR06VGPHjlXZsmXVv3//224nOTlZUVFRf3+P7mNJSUkPfA+cgb46B311HnrrHPRV8vb2ztZ80dHRqlatmuP28uXLJUkBAQGOaYMGDdKQIUPk6+ur8+fPS5KqVaum8PBwjRs3zrGMJG3evFn79+/X4MGDHdPGjx+vLl26pNtO48aNNWfOHA0aNEibN292TD9w4ICWLVum8ePHO6bNnj1b//jHP9S4cWPHtICAAL355pvq3LmzDhw4IEkqWbKktmzZolmzZmnOnDnsE/uUK/dp1KhR6tmzZ67ap9z4OLFP7NOf98kYo4zYTGb3/J+UlBTZ7Xbly5dP0vXz0ePi4rRp0yZJ0sKFC3Xt2jX169fPscz8+fNVt25dff755xo9erRmzZqlwMBAeXl5ZbqdqKiobL9xyK3ogXPQV+egr85Db52DvmYfvaIHzkJfnYO+Og+9dQ76mrksr+IeHh6uqVOnSpLOnTunxMREeXh46PTp0zLG6IcfflC9evUc88fGxurEiROqV6+eEhMT5erqKpvNpsTEROftBQAAAAAA97ksD3Hv3LmzQkJC1L17d9lsNk2ePFkuLi567bXXlJaWpkaNGqlmzZqO+efOnauXXnpJktSjRw/169dPpUuXVtWqVZ23FwAAAAAA3OeyDOh58+bV22+/fcv0ZcuWZTj/6NGjHX/7+PjIx8fnb5QHAAAAAMCDIctD3AEAAAAAgPMR0AEAAAAAsAACOgAAAAAAFkBABwAAAADAAgjoAAAAAABYAAEdAAAAAAALIKADAAAAAGABBHQAAAAAACyAgA4AAAAAgAUQ0AEAAAAAsAACOgAAAAAAFkBABwAAAADAAgjoAAAAAABYAAEdAAAAAAALIKADAAAAAGABBHQAAAAAACyAgA4AAAAAgAUQ0AEAAAAAsAACOgAAAAAAFkBABwAAAADAAgjoAAAAAABYAAEdAAAAAAALIKADAAAAAGABBHQAAAAAACyAgA4AAAAAgAUQ0AEAAAAAsAACOgAAAAAAFkBABwAAAADAAgjoAAAAAABYAAEdAAAAAAALIKADAAAAAGABBHQAAAAAACyAgA4AAAAAgAVkK6DHxsbK19dXx44d0/79+9W5c2f16NFDEydOlN1ul91u16BBgxQQEKBt27ZJkqKjozVp0iSnFg8AAAAAQG6RZUBPTU3V2LFjlS9fPknSmDFjNHLkSH3++efy9PTU6tWrFRUVpUcffVQffvihPvvsM0nSnDlzNHDgQOdWDwAAAABALpFlQJ82bZq6deumhx56SJJ07tw51alTR5JUp04d7d69Wx4eHkpOTlZSUpI8PDy0e/dulS9fXiVKlHBu9QAAAAAA5BJut7tzxYoVKlasmHx8fLRgwQJJUtmyZbVz5049+eST+u6775SYmCgvLy89/PDDmj59ugYNGqT33ntPI0aM0Lhx41S4cGEFBQXJxeX2nwUkJycrKirq7u3ZfSgpKemB74Ez0FfnoK/OQ2+dg75K3t7e2ZqPMZnni7PQV+egr85Db52DvmY+JtuMMSazhXr27CmbzSabzaaoqCiVL19er7/+uubPn69r166pXr16unLlikaOHOlYZvXq1bLb7Tp69KiaN2+unTt3qmrVqmrYsOFtC4yKisr2G4fcih44B311DvrqPPTWOehr9tEreuAs9NU56Kvz0FvnoK+Zu+3X2osWLdJnn32msLAweXt7a9q0adq/f79CQ0O1cOFCXb58OV3wTk5O1vr16+Xv76/ExES5urrKZrMpISHB6TsCAAAAAMD97LaHuGekXLly6tOnj/Lnz6+nnnpKvr6+jvsWLlyowMBA2Ww2derUSWPHjpWnp6dmz559V4sGAAAAACC3yXZADwsLkyRVqFBBTZs2zXCeAQMGOP729vbW8uXL/2Z5AAAAAAA8GLL1O+gAAAAAAMC5COgAAAAAAFgAAR0AAAAAAAsgoAMAAAAAYAEEdAAAAAAALICADgAAAACABRDQAQAAAACwAAI6AAAAAAAWQEAHAAAAAMACCOgAAAAAAFgAAR0AAAAAAAsgoAMAAAAAYAEEdAAAAAAALICADgAAAACABRDQAQAAAACwAAI6AAAAAAAWQEAHAAAAAMACCOgAAAAAAFgAAR0AAAAAAAsgoAMAAAAAYAEEdAAAAAAALICADgAAAACABRDQAQAAAACwAAI6AAAAAAAWQEAHAAAAAMACCOgAAAAAAFgAAR0AAAAAAAsgoAMAAAAAYAEEdAAAAAAALICADgAAAACABRDQAQAAAACwAAI6AAAAAAAWQEAHAAAAAMACshXQY2Nj5evrq2PHjikqKkpdunRR9+7dFRISIrvdLkkaO3asunTpoi+++EKSdOXKFb322mtOKxwAAAAAgNwky4CempqqsWPHKl++fJKkWbNmafDgwVq8eLFSUlK0efNmXbp0SRcuXNCSJUsUEREhSZo/f74GDBjg3OoBAAAAAMgl3LKaYdq0aerWrZsWLFggSfL29tbly5dljNHVq1fl5uYmd3d3paWlKTU1VXnz5lV0dLQSExNVuXLlbBeSnJysqKiov74nuUBSUtID3wNnoK/OQV+dh946B329PoZnB2Myzxdnoa/OQV+dh946B33NfEy+bUBfsWKFihUrJh8fH0dAL1++vCZMmKC5c+eqYMGCeuqpp+Tu7q4mTZro9ddf15AhQzR37ly9+OKLmjRpklxcXBQUFCQPD4/bFuju7p7tNw65VVRU1APfA2egr85BX52H3joHfc0+xmSeL85CX52DvjoPvXUO+pq52wb0iIgI2Ww2RUZGKioqSm+88YYOHjyolStXqlKlSlq0aJGmTp2qcePGqVu3burWrZv27NmjsmXLKjIyUvXq1ZMkrVmzRl26dLknOwQAAAAAwP3otuegL1q0SJ999pnCwsLk7e2tadOmqUyZMvL09JQkPfTQQ/rjjz/SLfPJJ5/ohRdeUFJSklxdXWWz2ZSQkOC8PQAAAAAAIBfI8hz0m02aNEnDhg2Tm5ub8uTJo4kTJzruW7t2rZo0aaJ8+fKpZcuWCgoKkouLi2bMmHFXiwYAAAAAILfJdkAPCwtz/L1kyZIM52nTpo3j70ceeSTT+QAAAAAAQHrZ+h10AAAAAADgXAR0AAAAAAAsgIAOAAAAAIAFENABAAAAALAAAjoAAAAAABZAQAcAAAAAwAII6AAAAAAAWAABHQAAAAAACyCgAwAAAABgAQR0AAAAAAAsgIAOAAAAAIAFENABAAAAALAAAjoAAAAAABZAQAcAAAAAwAII6AAAAAAAWAABHQAAAAAACyCgAwAAAABgAQR0AAAAAAAsgIAOAAAAAIAFENABAAAAALAAAjoAAAAAABZAQAcAAAAAwAII6AAAAAAAWAABHQAAAAAACyCgAwAAAABgATZjjMnpIiRp7969cnd3z+kyAADItdzc3FSpUqUs52NMBgDAuTIbky0T0AEAAAAAeJBxiDsAAAAAABZAQAcAAAAAwAII6AAAAAAAWAABHQAAAAAACyCgAwAAAABgAQR0AAAAAAAsgIBuEZs2bVKnTp3UtWtXLVu2LKfLyTXo691nt9s1duxYde3aVYGBgTp16lROl5Qr0Ffnobe4U4wdzkFf7z5e35yDvjoPvc0aAd0CUlNTNWXKFH388ccKCwvT0qVLdeHChZwu675HX51jw4YNSklJ0dKlSzV8+HBNnTo1p0vKFeir89Bb3AnGDuegr87B65tz0FfnobdZI6BbwLFjx/TYY4+pcOHCyps3r+rWratdu3bldFn3PfrqHLt375aPj48kqVatWvrtt99yuKLcgb46D73FnWDscA766hy8vjkHfXUeeps1AroFxMfHq2DBgo7bBQoUUHx8fA5WlDvQV+eIj4+Xp6en47arq6uuXbuWgxXlDvTVeegt7gRjh3PQV+fg9c056Kvz0NusueV0AQ+yGTNmaM+ePTp06JBq1KjhmH716tV0gxjuDH11Lk9PT129etVx2263y82Nl5K/i746D71FdjB2OAd9dS5e35yDvjoPvc0a36DnoGHDhiksLEzbtm3T6dOndfnyZaWkpOinn35S7dq1c7q8+xZ9da46dero+++/lyTt3btXlStXzuGKcgf66jz0FtnB2OEc9NW5eH1zDvrqPPQ2azZjjMnpInD9yqazZ8+WMUadOnVSz549c7qkXIG+3n12u13jx4/X4cOHZYzR5MmTVaFChZwu675HX52H3uJOMXY4B329+3h9cw766jz0NmsEdAAAAAAALIBD3AEAAAAAsAACOgAAAAAAFkBABwAAAADAAgjoAAAAAABYAAEdAAAAAAAL4FfhgQfQjh07FBQUpIoVKzqmFS1aVO+///4t80ZFRWnjxo0aMmTIX95ew4YNtW3btr+8PAAAuRVjMoA/I6ADD6j69etrxowZWc7n7e0tb2/ve1ARAAAPJsZkADcQ0AE4BAYGysvLSydOnJAxRjNmzNDx48e1ZMkSzZgxQyEhITp16pSSkpL0/PPPq3379tq2bZveffddubu7q0iRIpo8ebIKFCigMWPG6OjRoypbtqxSUlIkSWfPntWYMWOUnJwsd3d3TZw4UaVKlcrhvQYAwHoYk4EHEwEdeEBt375dgYGBjtu+vr6SpDp16mjChAlatGiR5s+fr+eee06SFB8fr127dmnZsmWSpG3btskYozFjxmjx4sV6+OGHtXDhQs2dO1c1a9ZUcnKyli1bpv/5n//RN998I0maNm2aAgMD5evrq8jISIWGhurtt9++x3sOAIC1MCYDuIGADjygMjqcbsuWLapfv76k628KNm3a5LjP09NTI0eO1JgxYxQfHy9/f39dunRJnp6eevjhhyVJ//znP/XOO++ocOHCqlGjhiSpdOnSjk/kDx8+rPnz5+vDDz+UMUZubrwEAQDAmAzgBv4TAaTz22+/6ZFHHtGePXvSXbAmJiZG+/fv1+zZs5WcnCxfX1/5+/srPj5eMTExeuihh7Rz506VL19eFStW1Nq1a9W7d2+dO3dO586dkyQ9/vjj6tu3r+rUqaNjx45p165dObWbAABYHmMy8OAhoAMPqJsPp5OkpKQkrVy5Up988ony58+v6dOn6/Dhw5KkkiVL6vz58+rWrZtcXFzUt29f5cmTR5MmTdLQoUNls9lUuHBhTZkyRUWLFtW2bdsUEBCg0qVLq2jRopKkN954Q+PHj1dycrKSkpI0atSoe77fAABYDWMygBtsxhiT00UAsIbAwECNHz9eFSpUyOlSAAB4oDEmAw8ml5wuAAAAAAAA8A06AAAAAACWwDfoAAAAAABYAAEdAAAAAAALIKADAAAAAGABBHQAAAAAACyAgA4AAAAAgAX8L0iBiSfMKZp9AAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "df1 = (results[['TrainAgent', 'TrainMarket']]\n",
    "       .sub(1)\n",
    "       .rolling(100)\n",
    "       .mean())\n",
    "df1.plot(ax=axes[0],\n",
    "         title='Annual Returns (Moving Average)',\n",
    "         lw=1)\n",
    "\n",
    "df2 = results['Strategy Wins (%)'].div(100).rolling(50).mean()\n",
    "df2.plot(ax=axes[1],\n",
    "         title='Agent Outperformance (%, Moving Average)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.yaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))\n",
    "axes[1].axhline(.5, ls='--', c='k', lw=1)\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'performance', dpi=300)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}