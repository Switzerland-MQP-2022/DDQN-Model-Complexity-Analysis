{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook documents the analysis of a variety of reinforcement learning models dedicated to SPDR\n",
    "trading.  These models all use the same algorithms and structure, a Double Deep Q Learning model, but where\n",
    "they differ is in the state space, or environment.  This projects goal is to analyse how increasing the\n",
    "complexity of the model's state space affects the performance of the model. This is an interesting topic\n",
    "because naturally state spaces can be too simple and not be able to learn, but due to the curse of\n",
    "dimensionality, if a state space gets too complicated we expect the model to over fit and perform\n",
    "poorly again.  This notebook stores all the different state spaces we tried and allow the user to select a\n",
    "state space to train themselves. We are letting the use select which one to train because training 6 models\n",
    "all at once will take days."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "Advances in machine learning and artificial intelligence (AI) have enabled us to enhance our lives and tackle a variety of\n",
    "complex problems. The financial market is a prime example of a field where researchers are employing these\n",
    "techniques. Since the financial market is very dynamic and ever fluctuating, it presents a unique challenges to consider\n",
    "when developing these systems, but also allows the power of machine learning and AI to shine. Before the\n",
    "development of AI, it was the job of investors and traders to use market data to make optimal\n",
    "decisions that maximize and reduce risk within the context of a trading system. However, due to market\n",
    "complexities, it can be challenging for agents to consider all the relevant information to take an informed\n",
    "position. This is where reinforcement learning (RL), an area of machine learning, comes into play. Through\n",
    "repeated interaction with a market environment, an RL agent can learn optimal trading strategies by taking\n",
    "certain actions, receiving rewards based on these, and adapting future actions based on previous experience.\n",
    "\n",
    "Reinforcement Learning has a rich history of use in the realm of finance. In the 1990s, Moody and Saffell experimented\n",
    "with real-time recurrent learning in order to demonstrate a predictable structure to U.S. stock prices (Moody\n",
    "& Saffell, 1998). They claimed that their agent was able to make a 4000% profit over the simulated period of\n",
    "1970 to 1994, far outperforming the S&P 500 stock index during the same timespan.\n",
    "\n",
    "However, previous studies into applying reinforcement learning into finance have provided insufficient analysis\n",
    "of their chosen model compared to similar ones. For instance, Wu et al. came up with their own technical\n",
    "indicators to add to their reinforcement model  [233]. However, they did not test their model against simpler\n",
    "models, they only tested it against the turtle trading strategy[256], a simple rule based strategy.\n",
    "This is an issue due to the well-studied phenomenon known as the “curse of\n",
    "dimensionality.” Simply put, as one adds more dimensions to a dataset with a fixed number of data points, the\n",
    "density of the data points gets smaller and thus it becomes harder to prevent models from overfitting. Somewhat\n",
    "paradoxically, this could lead to more complex models performing worse than simpler ones. Thus, it is important\n",
    "to test the model on multiple dimensionalities of data, to make sure the data is not too complex that it overfits,\n",
    "or too simple that it can’t learn enough.\n",
    "\n",
    "Since these papers do not provide an in-depth analysis, this notebook analyses how altering\n",
    "the complexity of data available to a trading agent affects its overall performance relative to the market. To\n",
    "do this, this notebook adopts a DDQN algorithm to trade in three environments, each focusing on one of equity indices, foreign\n",
    "exchange (Forex), and market3. Each market environment contains multiple state spaces with varying amounts of data\n",
    "and asset dimensionality, such as 1-Day returns, 5-Day returns, currencies and market3example.\n",
    " The user can then decide which dataset and state\n",
    "space to train, thus seeing how well each model performs, and which amount of dimentionality is the best.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Description\n",
    "\n",
    "There are 3 data sets we are using for these models, one for equity indexes, one for the foreign exchange market,\n",
    "and one for _____.  These data sets were collected from Refinitiv, and they consist of the daily closing prices of\n",
    "each asset we were using.  For the equity indexes we naturally have the daily close prices of SPY, as that is the\n",
    "index we are predicting, we also have the daily close prices of NSDQ.O, DIA, GLD, and USO.  We are using NSDQ.O and\n",
    "DIA as they are similar to the SPY and could reasonably help predict SPY, this data is used in the 4th and 5th complex\n",
    "models.  We are using GLD and USO as they are further removed and actually do more harm than good and make the model\n",
    "preform worse, thus showing the curse of dimensionality.  That data is only used in the 5th model.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.634858Z",
     "start_time": "2021-02-25T06:20:27.942424Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import deque\n",
    "from random import sample\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.638316Z",
     "start_time": "2021-02-25T06:20:29.636097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "#Which model to run(0 - 6, 0 being the simplest, 6 being the most complex)\n",
    "model = 0\n",
    "\n",
    "#Random setup stuff\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "#Use a GPU is we have one\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "#Set up results directory\n",
    "results_path = Path('results', 'trading_bot')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.688161Z",
     "start_time": "2021-02-25T06:20:29.681742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Trading costs: 0.10% | Time costs: 0.01%'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simulation variables\n",
    "trading_days = 252\n",
    "trading_cost_bps = 1e-3\n",
    "time_cost_bps = 1e-4\n",
    "\n",
    "register(\n",
    "    id='trading-v0',\n",
    "    entry_point='trading_env:TradingEnvironment',\n",
    "    max_episode_steps=trading_days\n",
    "\n",
    ")\n",
    "\n",
    "f'Trading costs: {trading_cost_bps:.2%} | Time costs: {time_cost_bps:.2%}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.544485Z",
     "start_time": "2021-02-25T06:20:29.698083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trading_env:trading_env logger started.\n",
      "INFO:trading_env:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1550 entries, 22 to 1618\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   returns  1550 non-null   float64\n",
      " 1   ret_2    1550 non-null   float64\n",
      " 2   ret_5    1550 non-null   float64\n",
      " 3   ret_10   1550 non-null   float64\n",
      " 4   ret_21   1550 non-null   float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 72.7 KB\n"
     ]
    }
   ],
   "source": [
    "#Initalize environment\n",
    "trading_environment = gym.make('trading-v0', trading_days = trading_days, model = model)\n",
    "trading_environment.env.trading_days = trading_days\n",
    "trading_environment.env.data_source.trading_days = trading_days\n",
    "trading_environment.env.simulator.steps = trading_days\n",
    "\n",
    "trading_environment.env.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.simulator.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.simulator.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.simulator.reinitialize()\n",
    "trading_environment.env.ticker = 'AAPL'\n",
    "trading_environment.seed(42)\n",
    "\n",
    "# Get Environment Params\n",
    "state_dim = trading_environment.observation_space.shape[0]\n",
    "num_actions = trading_environment.action_space.n\n",
    "max_episode_steps = trading_environment.spec.max_episode_steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "## Define Trading Agent(he Neural Network)\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_dim,\n",
    "                 num_actions,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 replay_capacity,\n",
    "                 architecture,\n",
    "                 l2_reg,\n",
    "                 tau,\n",
    "                 batch_size):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.experience = deque([], maxlen=replay_capacity)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.architecture = architecture\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.online_network = self.build_model()\n",
    "        self.target_network = self.build_model(trainable=False)\n",
    "        self.update_target()\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
    "        self.epsilon_history = []\n",
    "\n",
    "        self.total_steps = self.train_steps = 0\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.losses = []\n",
    "        self.idx = tf.range(batch_size)\n",
    "        self.train = True\n",
    "\n",
    "    def build_model(self, trainable=True):\n",
    "        layers = []\n",
    "        n = len(self.architecture)\n",
    "        for i, units in enumerate(self.architecture, 1):\n",
    "            layers.append(Dense(units=units,\n",
    "                                input_dim=self.state_dim if i == 1 else None,\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=l2(self.l2_reg),\n",
    "                                name=f'Dense_{i}',\n",
    "                                trainable=trainable))\n",
    "        layers.append(Dropout(.1))\n",
    "        layers.append(Dense(units=self.num_actions,\n",
    "                            trainable=trainable,\n",
    "                            name='Output'))\n",
    "        model = Sequential(layers)\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_network.set_weights(self.online_network.get_weights())\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps += 1\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        q = self.online_network.predict(state)\n",
    "        return np.argmax(q, axis=1).squeeze()\n",
    "\n",
    "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
    "        if not_done:\n",
    "            self.episode_reward += r\n",
    "            self.episode_length += 1\n",
    "        else:\n",
    "            if self.train:\n",
    "                if self.episodes < self.epsilon_decay_steps:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                else:\n",
    "                    self.epsilon *= self.epsilon_exponential_decay\n",
    "\n",
    "            self.episodes += 1\n",
    "            self.rewards_history.append(self.episode_reward)\n",
    "            self.steps_per_episode.append(self.episode_length)\n",
    "            self.episode_reward, self.episode_length = 0, 0\n",
    "\n",
    "        self.experience.append((s, a, r, s_prime, not_done))\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if self.batch_size > len(self.experience):\n",
    "            return\n",
    "        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n",
    "        states, actions, rewards, next_states, not_done = minibatch\n",
    "\n",
    "        next_q_values = self.online_network.predict_on_batch(next_states)\n",
    "        best_actions = tf.argmax(next_q_values, axis=1)\n",
    "\n",
    "        next_q_values_target = self.target_network.predict_on_batch(next_states)\n",
    "        target_q_values = tf.gather_nd(next_q_values_target,\n",
    "                                       tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n",
    "\n",
    "        targets = rewards + not_done * self.gamma * target_q_values\n",
    "\n",
    "        q_values = self.online_network.predict_on_batch(states)\n",
    "        q_values[[self.idx, actions]] = targets\n",
    "\n",
    "        loss = self.online_network.train_on_batch(x=states, y=q_values)\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.575368Z",
     "start_time": "2021-02-25T06:20:32.565067Z"
    }
   },
   "outputs": [],
   "source": [
    "#RL hypers\n",
    "gamma = .99,  # discount factor\n",
    "tau = 100  # target network update frequency\n",
    "\n",
    "### NN Architecture\n",
    "\n",
    "architecture = (256, 256)  # units per layer\n",
    "learning_rate = 0.0001  # learning rate\n",
    "l2_reg = 1e-6  # L2 regularization\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "replay_capacity = int(1e6)\n",
    "batch_size = 4096\n",
    "\n",
    "### epsilon-greedy Policy\n",
    "\n",
    "epsilon_start = 1.0 # starting point for epsilon\n",
    "epsilon_end = .01 # ending point for epsilon\n",
    "epsilon_decay_steps = 250 # the number of steps to get from start to end\n",
    "epsilon_exponential_decay = .99 # after 250 step(epsilon_decay_steps) epsilon = epsilon*epsilon_exponential_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.613239Z",
     "start_time": "2021-02-25T06:20:32.604766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_1 (Dense)             (None, 256)               1536      \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,099\n",
      "Trainable params: 68,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#clear out karas\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#instantiate the ddqn model\n",
    "ddqn = DDQNAgent(state_dim=state_dim,\n",
    "                 num_actions=num_actions,\n",
    "                 learning_rate=learning_rate,\n",
    "                 gamma=gamma,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                 replay_capacity=replay_capacity,\n",
    "                 architecture=architecture,\n",
    "                 l2_reg=l2_reg,\n",
    "                 tau=tau,\n",
    "                 batch_size=batch_size)\n",
    "\n",
    "ddqn.online_network.summary()\n",
    "\n",
    "### Set Experiment parameters\n",
    "\n",
    "total_steps = 0\n",
    "max_episodes = 20\n",
    "\n",
    "### Initialize Experiment variables\n",
    "\n",
    "episode_time, navs, market_navs, diffs, episode_eps = [], [], [], [], []\n",
    "test_navs, test_market_navs, test_diffs = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.752721Z",
     "start_time": "2021-02-25T06:20:32.742471Z"
    }
   },
   "outputs": [],
   "source": [
    "#prints the results from the training and testing runs\n",
    "def track_results(episode, nav_ma_100, nav_ma_10,\n",
    "                  market_nav_100, market_nav_10,\n",
    "                  win_ratio, total, epsilon, pretext=\"Training Results:\"):\n",
    "    time_ma = np.mean([episode_time[-100:]])\n",
    "    T = np.sum(episode_time)\n",
    "    \n",
    "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
    "    print(pretext + template.format(episode, format_time(total),\n",
    "                          nav_ma_100-1, nav_ma_10-1, \n",
    "                          market_nav_100-1, market_nav_10-1, \n",
    "                          win_ratio, epsilon))\n",
    "\n",
    "#Runs a year long simulation on the testing data\n",
    "def test_data_simulation():\n",
    "    #reset the environment\n",
    "    testthis_state = trading_environment.reset(training=False)\n",
    "\n",
    "    #loop for a year\n",
    "    for test_episode_step in range(max_episode_steps):\n",
    "        testaction = ddqn.epsilon_greedy_policy(testthis_state.reshape(-1, state_dim))\n",
    "        testnext_state, testreward, testdone, _ = trading_environment.step(testaction)\n",
    "\n",
    "        if testdone:\n",
    "            break\n",
    "        testthis_state = testnext_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # get DataFrame with seqence of actions, returns and nav values\n",
    "    test_result = trading_environment.env.simulator.result()\n",
    "\n",
    "    # get results of last step\n",
    "    test_final = test_result.iloc[-1]\n",
    "\n",
    "\n",
    "    # apply return (net of cost) of last action to last starting nav\n",
    "    test_nav = test_final.nav * (1 + test_final.strategy_return)\n",
    "    test_navs.append(test_nav)\n",
    "\n",
    "    # market nav\n",
    "    test_market_nav = test_final.market_nav\n",
    "    test_market_navs.append(test_market_nav)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    test_diff = test_nav - test_market_nav\n",
    "    test_diffs.append(test_diff)\n",
    "\n",
    "    #Store the results\n",
    "    track_results(episode,\n",
    "                  # show mov. average results for 100 (10) periods\n",
    "                  np.mean(test_navs[-100:]),\n",
    "                  np.mean(test_navs[-10:]),\n",
    "                  np.mean(test_market_navs[-100:]),\n",
    "                  np.mean(test_market_navs[-10:]),\n",
    "                  # share of agent wins, defined as higher ending nav\n",
    "                  np.sum([s > 0 for s in test_diffs[-100:]])/min(len(test_diffs), 100),\n",
    "                  time() - start, -1, pretext=\"Testing Results:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Results:   1 | 00:00:00 | Agent:  11.5% ( 11.5%) | Market:   8.7% (  8.7%) | Wins: 100.0% | eps: -1.000\n",
      "[-0.00542352 -0.51763722 -0.18033141 -0.28462506 -0.6085253 ]\n",
      "Testing Results:   2 | 00:00:00 | Agent:   6.3% (  6.3%) | Market:   5.1% (  5.1%) | Wins: 50.0% | eps: -1.000\n",
      "[-0.01425776  1.44017732  1.65037304  1.28394585  1.04452728]\n",
      "Testing Results:   3 | 00:00:00 | Agent:   6.2% (  6.2%) | Market:   2.9% (  2.9%) | Wins: 66.7% | eps: -1.000\n",
      "[ 0.01236975  0.71842311  1.06173007  0.18289755 -0.418968  ]\n",
      "Testing Results:   4 | 00:00:01 | Agent:  -3.1% ( -3.1%) | Market:   1.5% (  1.5%) | Wins: 50.0% | eps: -1.000\n",
      "[-0.00605841 -1.22869006  1.72565464  1.6104793   2.0046518 ]\n",
      "Testing Results:   5 | 00:00:01 | Agent:  -8.0% ( -8.0%) | Market:   0.3% (  0.3%) | Wins: 40.0% | eps: -1.000\n",
      "[-0.00944172 -0.47502813 -0.86443098 -0.39785279  0.67605101]\n",
      "Testing Results:   6 | 00:00:01 | Agent: -10.6% (-10.6%) | Market:   1.8% (  1.8%) | Wins: 33.3% | eps: -1.000\n",
      "[ 0.00763004  0.29873284 -0.15787609 -0.43524085  0.18464516]\n",
      "Testing Results:   7 | 00:00:02 | Agent: -13.8% (-13.8%) | Market:   1.6% (  1.6%) | Wins: 28.6% | eps: -1.000\n",
      "[-0.01032891 -0.81744224  0.88958936 -0.30442768  1.27879722]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16348/622760671.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthis_state\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mepisode_step\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmax_episode_steps\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[0maction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mddqn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepsilon_greedy_policy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthis_state\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m         \u001B[0mnext_state\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrading_environment\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16348/1816324124.py\u001B[0m in \u001B[0;36mepsilon_greedy_policy\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrand\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepsilon\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchoice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_actions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 73\u001B[1;33m         \u001B[0mq\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0monline_network\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     74\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mq\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 64\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1756\u001B[0m               stacklevel=2)\n\u001B[0;32m   1757\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1758\u001B[1;33m       data_handler = data_adapter.get_data_handler(\n\u001B[0m\u001B[0;32m   1759\u001B[0m           \u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1760\u001B[0m           \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36mget_data_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1401\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"model\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"_cluster_coordinator\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1402\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0m_ClusterCoordinatorDataHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1403\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mDataHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1404\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1405\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[0;32m   1151\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1152\u001B[0m     \u001B[0madapter_cls\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mselect_data_adapter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1153\u001B[1;33m     self._adapter = adapter_cls(\n\u001B[0m\u001B[0;32m   1154\u001B[0m         \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1155\u001B[0m         \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001B[0m\n\u001B[0;32m    289\u001B[0m     \u001B[1;31m# trigger the next permutation. On the other hand, too many simultaneous\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    290\u001B[0m     \u001B[1;31m# shuffles can contend on a hardware level and degrade all performance.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 291\u001B[1;33m     \u001B[0mindices_dataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mindices_dataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpermutation\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprefetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    292\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    293\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mslice_batch_indices\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36mmap\u001B[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001B[0m\n\u001B[0;32m   2002\u001B[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001B[0;32m   2003\u001B[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001B[1;32m-> 2004\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mMapDataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmap_func\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpreserve_cardinality\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2005\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2006\u001B[0m       return ParallelMapDataset(\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001B[0m\n\u001B[0;32m   5453\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_use_inter_op_parallelism\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0muse_inter_op_parallelism\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5454\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_preserve_cardinality\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpreserve_cardinality\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 5455\u001B[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001B[0m\u001B[0;32m   5456\u001B[0m         \u001B[0mmap_func\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   5457\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_transformation_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001B[0m\n\u001B[0;32m   4531\u001B[0m         \u001B[0mfn_factory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrace_tf_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdefun_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4532\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4533\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfn_factory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4534\u001B[0m     \u001B[1;31m# There is no graph to add in eager mode.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4535\u001B[0m     \u001B[0madd_to_graph\u001B[0m \u001B[1;33m&=\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mget_concrete_function\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3242\u001B[0m          \u001B[1;32mor\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;31m`\u001B[0m \u001B[1;32mor\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensorSpec\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3243\u001B[0m     \"\"\"\n\u001B[1;32m-> 3244\u001B[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001B[0m\u001B[0;32m   3245\u001B[0m         *args, **kwargs)\n\u001B[0;32m   3246\u001B[0m     \u001B[0mgraph_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_garbage_collector\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelease\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_get_concrete_function_garbage_collected\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3208\u001B[0m       \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3209\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3210\u001B[1;33m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3211\u001B[0m       \u001B[0mseen_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3212\u001B[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m   3555\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3556\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3557\u001B[1;33m           \u001B[0mgraph_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3558\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3559\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m   3390\u001B[0m     \u001B[0marg_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbase_arg_names\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mmissing_arg_names\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3391\u001B[0m     graph_function = ConcreteFunction(\n\u001B[1;32m-> 3392\u001B[1;33m         func_graph_module.func_graph_from_py_func(\n\u001B[0m\u001B[0;32m   3393\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3394\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001B[0m\n\u001B[0;32m   1025\u001B[0m     \u001B[1;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop_return_value\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mop_return_value\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1026\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mfunc_graph\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1027\u001B[1;33m     func_graph = FuncGraph(name, collections=collections,\n\u001B[0m\u001B[0;32m   1028\u001B[0m                            capture_by_value=capture_by_value)\n\u001B[0;32m   1029\u001B[0m   \u001B[1;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mFuncGraph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, name, collections, capture_by_value)\u001B[0m\n\u001B[0;32m    188\u001B[0m         \u001B[1;32mfrom\u001B[0m \u001B[0mouter\u001B[0m \u001B[0mgraphs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mfailing\u001B[0m \u001B[0mthat\u001B[0m \u001B[0mwill\u001B[0m \u001B[0mdefault\u001B[0m \u001B[0mto\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m     \"\"\"\n\u001B[1;32m--> 190\u001B[1;33m     \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mFuncGraph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    191\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    192\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   3111\u001B[0m     \u001B[1;31m# TODO(skyewm): fold as much of the above as possible into the C\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3112\u001B[0m     \u001B[1;31m# implementation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3113\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_scoped_c_graph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mc_api_util\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mScopedTFGraph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3114\u001B[0m     \u001B[1;31m# The C API requires all ops to have shape functions. Disable this\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3115\u001B[0m     \u001B[1;31m# requirement (many custom ops do not have shape functions, and we don't\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     49\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 50\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgraph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mc_api\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_NewGraph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     51\u001B[0m     \u001B[1;31m# Note: when we're destructing the global context (i.e when the process is\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m     \u001B[1;31m# terminating) we may have already deleted other modules. By capturing the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []\n",
    "for episode in range(1, max_episodes + 1):\n",
    "    test_data_simulation()\n",
    "    this_state = trading_environment.reset()\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "        next_state, reward, done, _ = trading_environment.step(action)\n",
    "\n",
    "        ddqn.memorize_transition(this_state, \n",
    "                                 action, \n",
    "                                 reward, \n",
    "                                 next_state, \n",
    "                                 0.0 if done else 1.0)\n",
    "        if ddqn.train:\n",
    "            ddqn.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "        this_state = next_state\n",
    "\n",
    "    # get DataFrame with seqence of actions, returns and nav values\n",
    "    result = trading_environment.env.simulator.result()\n",
    "    \n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # apply return (net of cost) of last action to last starting nav \n",
    "    nav = final.nav * (1 + final.strategy_return)\n",
    "    navs.append(nav)\n",
    "\n",
    "    # market nav \n",
    "    market_nav = final.market_nav\n",
    "    market_navs.append(market_nav)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    diff = nav - market_nav\n",
    "    diffs.append(diff)\n",
    "    if episode % 10 == 0:\n",
    "        track_results(episode, \n",
    "                      # show mov. average results for 100 (10) periods\n",
    "                      np.mean(navs[-100:]), \n",
    "                      np.mean(navs[-10:]), \n",
    "                      np.mean(market_navs[-100:]), \n",
    "                      np.mean(market_navs[-10:]), \n",
    "                      # share of agent wins, defined as higher ending nav\n",
    "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100), \n",
    "                      time() - start, ddqn.epsilon)\n",
    "\n",
    "    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n",
    "        print(result.tail())\n",
    "        break\n",
    "\n",
    "print(\"final\")\n",
    "track_results(episode,\n",
    "              # show mov. average results for 100 (10) periods\n",
    "              np.mean(navs[-100:]),\n",
    "              np.mean(navs[-10:]),\n",
    "              np.mean(market_navs[-100:]),\n",
    "              np.mean(market_navs[-10:]),\n",
    "              # share of agent wins, defined as higher ending nav\n",
    "              np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
    "              time() - start, ddqn.epsilon)\n",
    "test_data_simulation()\n",
    "trading_environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "         NumStateVars  TrainAgent  TrainMarket  TrainDifference\n",
      "Episode                                                        \n",
      "1                   1    0.906501     0.922871        -0.016369\n",
      "2                   1    0.842965     1.067782        -0.224816\n",
      "3                   1    0.816793     0.832601        -0.015808\n",
      "4                   1    0.873289     0.911879        -0.038590\n",
      "5                   1    0.730276     0.852615        -0.122339\n",
      "6                   1    0.514472     1.011792        -0.497320\n",
      "7                   1    0.875691     0.838013         0.037679\n",
      "8                   1    1.032097     0.857081         0.175016\n",
      "9                   1    0.755167     1.251581        -0.496414\n",
      "10                  1    0.654865     1.064232        -0.409367\n",
      "11                  1    0.903980     0.932789        -0.028809\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11 entries, 1 to 11\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   NumStateVars       11 non-null     int64  \n",
      " 1   TrainAgent         11 non-null     float64\n",
      " 2   TrainMarket        11 non-null     float64\n",
      " 3   TrainDifference    11 non-null     float64\n",
      " 4   Strategy Wins (%)  0 non-null      float64\n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 528.0 bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10 entries, 1 to 10\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   NumStateVars       10 non-null     int64  \n",
      " 1   TestAgent          10 non-null     float64\n",
      " 2   TestMarket         10 non-null     float64\n",
      " 3   TestDifference     10 non-null     float64\n",
      " 4   Strategy Wins (%)  0 non-null      float64\n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 480.0 bytes\n"
     ]
    }
   ],
   "source": [
    "print(len(diffs))\n",
    "\n",
    "exampleState = trading_environment.reset()\n",
    "numStateVars = len(exampleState)\n",
    "\n",
    "results = pd.DataFrame({'NumStateVars': numStateVars,\n",
    "                        'Episode': list(range(1, episode+1)),\n",
    "                        'TrainAgent': navs,\n",
    "                        'TrainMarket': market_navs,\n",
    "                        'TrainDifference': diffs}).set_index('Episode')\n",
    "print(results)\n",
    "results['Strategy Wins (%)'] = (results.TrainDifference > 0).rolling(100).sum()\n",
    "results.info()\n",
    "\n",
    "test_results = pd.DataFrame({'NumStateVars': numStateVars,\n",
    "                        'EpisodeDiv10': list(range(1, len(test_navs)+1)),\n",
    "                        'TestAgent': test_navs,\n",
    "                        'TestMarket': test_market_navs,\n",
    "                        'TestDifference': test_diffs}).set_index('EpisodeDiv10')\n",
    "\n",
    "\n",
    "test_results['Strategy Wins (%)'] = (test_results.TestDifference > 0).rolling(100).sum()\n",
    "test_results.info()\n",
    "\n",
    "#Get the date and time so we can keep track of the data files\n",
    "currentTime = datetime.now()\n",
    "training_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TrainResults.csv'\n",
    "testing_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TestResults.csv'\n",
    "\n",
    "\n",
    "#store the results in a csv\n",
    "results.to_csv(results_path / training_file_name)\n",
    "test_results.to_csv(results_path / testing_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    sns.distplot(results.TrainDifference)\n",
    "    sns.despine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.031Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1008x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "df1 = (results[['TrainAgent', 'TrainMarket']]\n",
    "       .sub(1)\n",
    "       .rolling(100)\n",
    "       .mean())\n",
    "df1.plot(ax=axes[0],\n",
    "         title='Annual Returns (Moving Average)',\n",
    "         lw=1)\n",
    "\n",
    "df2 = results['Strategy Wins (%)'].div(100).rolling(50).mean()\n",
    "df2.plot(ax=axes[1],\n",
    "         title='Agent Outperformance (%, Moving Average)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.yaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))\n",
    "axes[1].axhline(.5, ls='--', c='k', lw=1)\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'performance', dpi=300)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}