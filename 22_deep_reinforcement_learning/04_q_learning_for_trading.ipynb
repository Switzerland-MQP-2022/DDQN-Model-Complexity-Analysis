{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Trading - Deep Q-learning & the stock market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a trading agent, we need to create a market environment that provides price and other information, offers trading-related actions, and keeps track of the portfolio to reward the agent accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Design an OpenAI trading environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI Gym allows for the design, registration, and utilization of environments that adhere to its architecture, as described in its [documentation](https://github.com/openai/gym/tree/master/gym/envs#how-to-create-new-environments-for-gym). The [trading_env.py](trading_env.py) file implements an example that illustrates how to create a class that implements the requisite `step()` and `reset()` methods.\n",
    "\n",
    "The trading environment consists of three classes that interact to facilitate the agent's activities:\n",
    " 1. The `DataSource` class loads a time series, generates a few features, and provides the latest observation to the agent at each time step. \n",
    " 2. `TradingSimulator` tracks the positions, trades and cost, and the performance. It also implements and records the results of a buy-and-hold benchmark strategy. \n",
    " 3. `TradingEnvironment` itself orchestrates the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book chapter explains these elements in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic trading game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agent, we need to set up a simple game with a limited set of options, a relatively low-dimensional state, and other parameters that can be easily modified and extended.\n",
    "\n",
    "More specifically, the environment samples a stock price time series for a single ticker using a random start date to simulate a trading period that, by default, contains 252 days, or 1 year. The state contains the (scaled) price and volume, as well as some technical indicators like the percentile ranks of price and volume, a relative strength index (RSI), as well as 5- and 21-day returns. The agent can choose from three actions:\n",
    "\n",
    "- **Buy**: Invest capital for a long position in the stock\n",
    "- **Flat**: Hold cash only\n",
    "- **Sell short**: Take a short position equal to the amount of capital\n",
    "\n",
    "The environment accounts for trading cost, which is set to 10bps by default. It also deducts a 1bps time cost per period. It tracks the net asset value (NAV) of the agent's portfolio and compares it against the market portfolio (which trades frictionless to raise the bar for the agent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same DDQN agent and neural network architecture that successfully learned to navigate the Lunar Lander environment. We let exploration continue for 500,000 time steps (~2,000 1yr trading periods) with linear decay of Îµ to 0.1 and exponential decay at a factor of 0.9999 thereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.634858Z",
     "start_time": "2021-02-25T06:20:27.942424Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import deque\n",
    "from random import sample\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.638316Z",
     "start_time": "2021-02-25T06:20:29.636097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# %%\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# %%\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "# %%\n",
    "\n",
    "results_path = Path('results', 'trading_bot')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)\n",
    "\n",
    "# %% md\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "# %%\n",
    "\n",
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the custom environment, just like with the Lunar Lander environment, we need to register it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.688161Z",
     "start_time": "2021-02-25T06:20:29.681742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Trading costs: 0.10% | Time costs: 0.01%'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_days = 252\n",
    "\n",
    "# %%\n",
    "\n",
    "register(\n",
    "    id='trading-v0',\n",
    "    entry_point='trading_env:TradingEnvironment',\n",
    "    max_episode_steps=trading_days\n",
    ")\n",
    "\n",
    "# %% md\n",
    "\n",
    "### Initialize Trading Environment\n",
    "\n",
    "# %% md\n",
    "\n",
    "## We can instantiate the environment by using the desired trading costs and ticker:\n",
    "\n",
    "# %%\n",
    "\n",
    "trading_cost_bps = 1e-3\n",
    "time_cost_bps = 1e-4\n",
    "\n",
    "# %%\n",
    "\n",
    "f'Trading costs: {trading_cost_bps:.2%} | Time costs: {time_cost_bps:.2%}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.544485Z",
     "start_time": "2021-02-25T06:20:29.698083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trading_env:trading_env logger started.\n",
      "INFO:trading_env:loading data for AAPL...\n",
      "INFO:trading_env:got data for AAPL...\n",
      "INFO:trading_env:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 9367 entries, (Timestamp('1981-01-30 00:00:00'), 'AAPL') to (Timestamp('2018-03-27 00:00:00'), 'AAPL')\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   returns  9367 non-null   float64\n",
      " 1   ret_2    9367 non-null   float64\n",
      " 2   ret_5    9367 non-null   float64\n",
      " 3   ret_10   9367 non-null   float64\n",
      " 4   ret_21   9367 non-null   float64\n",
      " 5   rsi      9367 non-null   float64\n",
      " 6   macd     9367 non-null   float64\n",
      " 7   atr      9367 non-null   float64\n",
      " 8   stoch    9367 non-null   float64\n",
      " 9   ultosc   9367 non-null   float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 1.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "[42]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_environment = gym.make('trading-v0')\n",
    "trading_environment.env.trading_days = trading_days\n",
    "trading_environment.env.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.ticker = 'AAPL'\n",
    "trading_environment.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.548145Z",
     "start_time": "2021-02-25T06:20:32.545830Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dim = trading_environment.observation_space.shape[0]\n",
    "num_actions = trading_environment.action_space.n\n",
    "max_episode_steps = trading_environment.spec.max_episode_steps\n",
    "\n",
    "\n",
    "# %% md\n",
    "\n",
    "## Define Trading Agent\n",
    "\n",
    "# %%\n",
    "\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_dim,\n",
    "                 num_actions,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 replay_capacity,\n",
    "                 architecture,\n",
    "                 l2_reg,\n",
    "                 tau,\n",
    "                 batch_size):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.experience = deque([], maxlen=replay_capacity)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.architecture = architecture\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.online_network = self.build_model()\n",
    "        self.target_network = self.build_model(trainable=False)\n",
    "        self.update_target()\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
    "        self.epsilon_history = []\n",
    "\n",
    "        self.total_steps = self.train_steps = 0\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.losses = []\n",
    "        self.idx = tf.range(batch_size)\n",
    "        self.train = True\n",
    "\n",
    "    def build_model(self, trainable=True):\n",
    "        layers = []\n",
    "        n = len(self.architecture)\n",
    "        for i, units in enumerate(self.architecture, 1):\n",
    "            layers.append(Dense(units=units,\n",
    "                                input_dim=self.state_dim if i == 1 else None,\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=l2(self.l2_reg),\n",
    "                                name=f'Dense_{i}',\n",
    "                                trainable=trainable))\n",
    "        layers.append(Dropout(.1))\n",
    "        layers.append(Dense(units=self.num_actions,\n",
    "                            trainable=trainable,\n",
    "                            name='Output'))\n",
    "        model = Sequential(layers)\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_network.set_weights(self.online_network.get_weights())\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps += 1\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        q = self.online_network.predict(state)\n",
    "        return np.argmax(q, axis=1).squeeze()\n",
    "\n",
    "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
    "        if not_done:\n",
    "            self.episode_reward += r\n",
    "            self.episode_length += 1\n",
    "        else:\n",
    "            if self.train:\n",
    "                if self.episodes < self.epsilon_decay_steps:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                else:\n",
    "                    self.epsilon *= self.epsilon_exponential_decay\n",
    "\n",
    "            self.episodes += 1\n",
    "            self.rewards_history.append(self.episode_reward)\n",
    "            self.steps_per_episode.append(self.episode_length)\n",
    "            self.episode_reward, self.episode_length = 0, 0\n",
    "\n",
    "        self.experience.append((s, a, r, s_prime, not_done))\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if self.batch_size > len(self.experience):\n",
    "            return\n",
    "        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n",
    "        states, actions, rewards, next_states, not_done = minibatch\n",
    "\n",
    "        next_q_values = self.online_network.predict_on_batch(next_states)\n",
    "        best_actions = tf.argmax(next_q_values, axis=1)\n",
    "\n",
    "        next_q_values_target = self.target_network.predict_on_batch(next_states)\n",
    "        target_q_values = tf.gather_nd(next_q_values_target,\n",
    "                                       tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n",
    "\n",
    "        targets = rewards + not_done * self.gamma * target_q_values\n",
    "\n",
    "        q_values = self.online_network.predict_on_batch(states)\n",
    "        q_values[[self.idx, actions]] = targets\n",
    "\n",
    "        loss = self.online_network.train_on_batch(x=states, y=q_values)\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.575368Z",
     "start_time": "2021-02-25T06:20:32.565067Z"
    }
   },
   "outputs": [],
   "source": [
    "gamma = .99,  # discount factor\n",
    "tau = 100  # target network update frequency\n",
    "\n",
    "# %% md\n",
    "\n",
    "### NN Architecture\n",
    "\n",
    "# %%\n",
    "\n",
    "architecture = (256, 256)  # units per layer\n",
    "learning_rate = 0.0001  # learning rate\n",
    "l2_reg = 1e-6  # L2 regularization\n",
    "\n",
    "# %% md\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "# %%\n",
    "\n",
    "replay_capacity = int(1e6)\n",
    "batch_size = 4096\n",
    "\n",
    "# %% md\n",
    "\n",
    "### $\\epsilon$-greedy Policy\n",
    "\n",
    "# %%\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = .01\n",
    "epsilon_decay_steps = 250\n",
    "epsilon_exponential_decay = .99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DDQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [TensorFlow](https://www.tensorflow.org/) to create our Double Deep Q-Network ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.613239Z",
     "start_time": "2021-02-25T06:20:32.604766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_1 (Dense)             (None, 256)               2816      \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 69,379\n",
      "Trainable params: 69,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# %%\n",
    "\n",
    "ddqn = DDQNAgent(state_dim=state_dim,\n",
    "                 num_actions=num_actions,\n",
    "                 learning_rate=learning_rate,\n",
    "                 gamma=gamma,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                 replay_capacity=replay_capacity,\n",
    "                 architecture=architecture,\n",
    "                 l2_reg=l2_reg,\n",
    "                 tau=tau,\n",
    "                 batch_size=batch_size)\n",
    "\n",
    "# %%\n",
    "\n",
    "ddqn.online_network.summary()\n",
    "\n",
    "# %% md\n",
    "\n",
    "## Run Experiment\n",
    "\n",
    "# %% md\n",
    "\n",
    "### Set parameters\n",
    "\n",
    "# %%\n",
    "\n",
    "total_steps = 0\n",
    "max_episodes = 50\n",
    "\n",
    "# %% md\n",
    "\n",
    "### Initialize variables\n",
    "\n",
    "# %%\n",
    "\n",
    "episode_time, navs, market_navs, diffs, episode_eps = [], [], [], [], []\n",
    "test_navs, test_market_navs, test_diffs = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.752721Z",
     "start_time": "2021-02-25T06:20:32.742471Z"
    }
   },
   "outputs": [],
   "source": [
    "def track_results(episode, nav_ma_100, nav_ma_10,\n",
    "                  market_nav_100, market_nav_10,\n",
    "                  win_ratio, total, epsilon, pretext=\"Training Results:\"):\n",
    "    time_ma = np.mean([episode_time[-100:]])\n",
    "    T = np.sum(episode_time)\n",
    "    \n",
    "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
    "    print(pretext + template.format(episode, format_time(total),\n",
    "                          nav_ma_100-1, nav_ma_10-1, \n",
    "                          market_nav_100-1, market_nav_10-1, \n",
    "                          win_ratio, epsilon))\n",
    "\n",
    "def test_data_simulation():\n",
    "    this_state = trading_environment.reset(training=False)\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "        next_state, reward, done, _ = trading_environment.step(action)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        this_state = next_state\n",
    "\n",
    "    # get DataFrame with seqence of actions, returns and nav values\n",
    "    result = trading_environment.env.simulator.result()\n",
    "\n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # apply return (net of cost) of last action to last starting nav\n",
    "    test_nav = final.nav * (1 + final.strategy_return)\n",
    "    test_navs.append(nav)\n",
    "\n",
    "    # market nav\n",
    "    market_nav = final.market_nav\n",
    "    test_market_navs.append(market_nav)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    diff = nav - market_nav\n",
    "    test_diffs.append(diff)\n",
    "\n",
    "    #Store the results\n",
    "    track_results(episode,\n",
    "                  # show mov. average results for 100 (10) periods\n",
    "                  np.mean(test_navs[-100:]),\n",
    "                  np.mean(test_navs[-10:]),\n",
    "                  np.mean(test_market_navs[-100:]),\n",
    "                  np.mean(test_market_navs[-10:]),\n",
    "                  # share of agent wins, defined as higher ending nav\n",
    "                  np.sum([s > 0 for s in test_diffs[-100:]])/min(len(test_diffs), 100),\n",
    "                  time() - start, ddqn.epsilon, pretext=\"Testing Results:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "going\n",
      "going\n",
      "going\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10812/2740281910.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"going\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0maction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mddqn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepsilon_greedy_policy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthis_state\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m         \u001B[0mnext_state\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrading_environment\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m         ddqn.memorize_transition(this_state, \n",
      "\u001B[1;32mc:\\users\\erich\\desktop\\pyvirtenvs\\mqpbook\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     16\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_elapsed_steps\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001B[1;32m---> 18\u001B[1;33m         \u001B[0mobservation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_elapsed_steps\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_elapsed_steps\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_max_episode_steps\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MQP\\TestCode\\Machine-Learning-for-Algorithmic-Trading-Second-Edition\\22_deep_reinforcement_learning\\trading_env.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    275\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction_space\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontains\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'{} {} invalid'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    276\u001B[0m         \u001B[0mobservation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_source\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtake_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 277\u001B[1;33m         reward, info = self.simulator.take_step(action=action,\n\u001B[0m\u001B[0;32m    278\u001B[0m                                                 market_return=observation[0])\n\u001B[0;32m    279\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mobservation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\MQP\\TestCode\\Machine-Learning-for-Algorithmic-Trading-Second-Edition\\22_deep_reinforcement_learning\\trading_env.py\u001B[0m in \u001B[0;36mtake_step\u001B[1;34m(self, action, market_return)\u001B[0m\n\u001B[0;32m    191\u001B[0m         \u001B[0mend_position\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maction\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;36m1\u001B[0m  \u001B[1;31m# short, neutral, long\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    192\u001B[0m         \u001B[0mn_trades\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mend_position\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mstart_position\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 193\u001B[1;33m         \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    194\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpositions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mend_position\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    195\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrades\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mn_trades\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []\n",
    "for episode in range(1, max_episodes + 1):\n",
    "    this_state = trading_environment.reset()\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "        next_state, reward, done, _ = trading_environment.step(action)\n",
    "\n",
    "        ddqn.memorize_transition(this_state, \n",
    "                                 action, \n",
    "                                 reward, \n",
    "                                 next_state, \n",
    "                                 0.0 if done else 1.0)\n",
    "        if ddqn.train:\n",
    "            ddqn.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "        this_state = next_state\n",
    "\n",
    "    # get DataFrame with seqence of actions, returns and nav values\n",
    "    result = trading_environment.env.simulator.result()\n",
    "    \n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # apply return (net of cost) of last action to last starting nav \n",
    "    nav = final.nav * (1 + final.strategy_return)\n",
    "    navs.append(nav)\n",
    "\n",
    "    # market nav \n",
    "    market_nav = final.market_nav\n",
    "    market_navs.append(market_nav)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    diff = nav - market_nav\n",
    "    diffs.append(diff)\n",
    "    if episode % 10 == 0:\n",
    "        track_results(episode, \n",
    "                      # show mov. average results for 100 (10) periods\n",
    "                      np.mean(navs[-100:]), \n",
    "                      np.mean(navs[-10:]), \n",
    "                      np.mean(market_navs[-100:]), \n",
    "                      np.mean(market_navs[-10:]), \n",
    "                      # share of agent wins, defined as higher ending nav\n",
    "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100), \n",
    "                      time() - start, ddqn.epsilon)\n",
    "        test_data_simulation()\n",
    "    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n",
    "        print(result.tail())\n",
    "        break\n",
    "\n",
    "print(\"final\")\n",
    "track_results(episode,\n",
    "              # show mov. average results for 100 (10) periods\n",
    "              np.mean(navs[-100:]),\n",
    "              np.mean(navs[-10:]),\n",
    "              np.mean(market_navs[-100:]),\n",
    "              np.mean(market_navs[-10:]),\n",
    "              # share of agent wins, defined as higher ending nav\n",
    "              np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
    "              time() - start, ddqn.epsilon)\n",
    "test_data_simulation()\n",
    "trading_environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'episode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10664/964413417.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m results = pd.DataFrame({'NumStateVars': numStateVars,\n\u001B[1;32m----> 7\u001B[1;33m                         \u001B[1;34m'Episode'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepisode\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m                         \u001B[1;34m'Agent'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mnavs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m                         \u001B[1;34m'Market'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmarket_navs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'episode' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(diffs))\n",
    "\n",
    "exampleState = trading_environment.reset()\n",
    "numStateVars = len(exampleState)\n",
    "\n",
    "results = pd.DataFrame({'NumStateVars': numStateVars,\n",
    "                        'Episode': list(range(1, episode+1)),\n",
    "                        'Train_Agent': navs,\n",
    "                        'Train_Market': market_navs,\n",
    "                        'Train_Difference': diffs}).set_index('Episode')\n",
    "\n",
    "results['Strategy Wins (%)'] = (results.Difference > 0).rolling(100).sum()\n",
    "results.info()\n",
    "\n",
    "test_results = pd.DataFrame({'NumStateVars': numStateVars,\n",
    "                        'EpisodeDiv10': list(range(1, len(test_navs)+1)),\n",
    "                        'Test_Agent': test_navs,\n",
    "                        'Test_Market': test_market_navs,\n",
    "                        'Test_Difference': test_diffs}).set_index('EpisodeDiv10')\n",
    "\n",
    "\n",
    "test_results['Strategy Wins (%)'] = (test_results.Difference > 0).rolling(100).sum()\n",
    "test_results.info()\n",
    "\n",
    "#Get the date and time so we can keep track of the data files\n",
    "currentTime = datetime.now()\n",
    "training_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TrainResults.csv'\n",
    "testing_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TestResults.csv'\n",
    "\n",
    "\n",
    "#store the results in a csv\n",
    "results.to_csv(results_path / training_file_name, index=False)\n",
    "test_results.to_csv(results_path / testing_file_name, index=False)\n",
    "\n",
    "#plot histogram\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    sns.distplot(results.Difference)\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1008x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "df1 = (results[['Agent', 'Market']]\n",
    "       .sub(1)\n",
    "       .rolling(100)\n",
    "       .mean())\n",
    "df1.plot(ax=axes[0],\n",
    "         title='Annual Returns (Moving Average)',\n",
    "         lw=1)\n",
    "\n",
    "df2 = results['Strategy Wins (%)'].div(100).rolling(50).mean()\n",
    "df2.plot(ax=axes[1],\n",
    "         title='Agent Outperformance (%, Moving Average)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.yaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))\n",
    "axes[1].axhline(.5, ls='--', c='k', lw=1)\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'performance', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This relatively simple agent uses **no information beyond the latest market data and the reward signal** compared to the machine learning models we covered elsewhere in this book. Nonetheless, it learns to make a profit and achieve performance similar to that of the market (after training on <1,000 years' worth of data, which takes only a fraction of the time on a GPU).\n",
    "\n",
    "Keep in mind that using a single stock also increases the **risk of overfitting** to the data â by a lot. You can test your trained agent on new data using the saved model (see the notebook for Lunar Lander).\n",
    "\n",
    "In summary, we have demonstrated the **mechanics of setting up an RL trading environment** and experimented with a basic agent that uses a small number of technical indicators. You should **try to extend both the environment and the agent** - for example, by allowing it to choose from several assets, size the positions, and manage risks.\n",
    "\n",
    "Reinforcement learning is often considered **one of the most promising approaches to algorithmic trading** because it most accurately models the task an investor is facing. However, our dramatically simplified examples illustrate that creating a **realistic environment poses a considerable challenge**. Moreover, deep reinforcement learning that has achieved impressive breakthroughs in other domains may face greater obstacles given the noisy nature of financial data, which makes it even harder to learn a value function based on delayed rewards.\n",
    "\n",
    "Nonetheless, the substantial interest in this subject makes it likely that institutional investors are working on larger-scale experiments that may yield tangible results. An interesting complementary approach beyond the scope of this book is **Inverse Reinforcement Learning**, which aims to identify the reward function of an agent(for example, a human trader) given its observed behavior; see [Arora and Doshi (2019)](https://www.semanticscholar.org/paper/A-Survey-of-Inverse-Reinforcement-Learning%3A-Methods-Arora-Doshi/9d4d8509f6da094a7c31e063f307e0e8592db27f) for a survey and [Roa-Vicens et al. (2019)](https://deepai.org/publication/towards-inverse-reinforcement-learning-for-limit-order-book-dynamics) for an application on trading in the limit-order book context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}