{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "This notebook documents the analysis of a variety of reinforcement learning models dedicated to trading of SPDR S&P 500 ETF Trust (SPY) stock.  These models all use the same algorithms and structure, a Double Deep Q Learning model, but differ in their state space, or environment.\n",
    "\n",
    "The goal of this project is to analyze how increasing the\n",
    "complexity of a model's state space affects the model's performance. This is an interesting topic because naturally state spaces may be too simple to properly learn, but due to the \"Curse of Dimensionality,\" if a state space gets too complicated, we can expect the model to overfit and possibly suffer worse performance.\n",
    "\n",
    "This notebook stores each of the state spaces we experimented with, and allows a user to select a state space to train themselves due to the long processing time required to train each state space at once."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Advances in machine learning (ML) and artificial intelligence (AI) have enabled us to enhance our lives and tackle a variety of\n",
    "complex problems. The financial market is a prime example of a field where researchers are employing these techniques. Since the financial market is very dynamic and ever fluctuating, it presents a unique challenges to consider when developing these systems, but also allows the power of machine learning and AI to shine. Before the development of AI, it was the job of investors and traders to use market data to make optimal decisions that maximize and reduce risk within the context of a trading system. However, due to market complexities, it can be challenging for agents to consider all the relevant information to take an informed position. This is where reinforcement learning (RL), an area of ML, comes into play. Through repeated interaction with a market environment, an RL agent can learn optimal trading strategies by taking certain actions, receiving rewards based on these, and adapting future actions based on previous experience.\n",
    "\n",
    "Reinforcement Learning has a rich history of use in the realm of finance. In the 1990s, Moody and Saffell experimented with real-time recurrent learning in order to demonstrate a predictable structure to U.S. stock prices (Moody & Saffell, 1998). They claimed that their agent was able to make a 4000% profit over the simulated period of 1970 to 1994, far outperforming the S&P 500 stock index during the same timespan.\n",
    "\n",
    "However, previous studies into applying reinforcement learning into finance have provided insufficient analysis of their chosen model compared to similar ones. For instance, Wu et al. came up with their own technical indicators to add to their reinforcement model [233]. However, they did not test their model against simpler models, they only tested it against the turtle trading strategy [256], a simple rule-based strategy. This is an issue due to the well-studied phenomenon known as the “curse of\n",
    "dimensionality.” Simply put, as one adds more dimensions to a dataset with a fixed number of data points, the density of the data points gets smaller and thus it becomes harder to prevent models from overfitting. Somewhat paradoxically, this could lead to more complex models performing worse than simpler ones. Thus, it is important to test the model on multiple dimensionalities of data, to make sure the data is not too complex that it overfits,\n",
    "or too simple that it can’t learn enough.\n",
    "\n",
    "Since these papers do not provide an in-depth analysis, this notebook analyses how altering the complexity of data available to a trading agent affects its overall performance relative to the market. To do this, this notebook adopts a DDQN algorithm to trade in three environments, each focusing on one of equity indices, foreign exchange (Forex), and market3. Each market environment contains multiple state spaces with varying amounts of data and asset dimensionality, such as 1-Day returns, 5-Day returns, currencies and market3example. The user can then decide which dataset and state space to train, thus seeing how well each model performs, and which amount of dimentionality is the best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <mark>For Documentation on Preparing the Program Environment see README.md </mark>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# update tables if needed\n",
    "! pip install --user --upgrade tables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Description\n",
    "\n",
    "We use three data sets for our models: one for equity indexes; one for the foreign exchange market;\n",
    "and one for equity indexes intraday trading, specifically 5 minute intervals. These data sets were collected from\n",
    "Refinitiv and Yahoo Finance, and they consist of the daily closing prices of\n",
    "their respective assets, except for FX data which instead uses the open price.\n",
    "\n",
    "This notebook uses the equity index intra day data.\n",
    "For the data, we have the prices of SPY (the index we are predicting), as well as the prices for\n",
    "NDAQ.O, DIA, GLD, and USO. We are using NDAQ.O and DIA as they are similar indices that could\n",
    "reasonably help to predict SPY. This data is used in our 5th and 6th models. We are then using GLD and\n",
    "USO as they are further removed from the SPY and actually make the model preform worse, thus showing\n",
    "the curse of dimensionality. That data is only used in our most complex environment, our 6th model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports & Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set Path and Read CSV File"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_STORE = Path('IndexIntradayAssets.h5')\n",
    "df = (pd.read_csv('IndexFundsDataIntraday.csv'))\n",
    "label = 'SAP'\n",
    "\n",
    "\n",
    "print(df.head(10))#make sure we got the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Store Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put(label, df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Connect and Store to Google Drive\n",
    "\n",
    "Allow this notebook to access your Google Drive when prompted, as that is where the data will be stored\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import deque\n",
    "from random import sample\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "\n",
    "# conect to google drive so we can store data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Methodology\n",
    "\n",
    "Since we are comparing the effectiveness of data with different dimensionalities we naturally have to train multiple models. The state variables for each model is shown in the table below.\n",
    "\n",
    "| Model 1 | Model 2 | Model 3 | Model 4 | Model 5 | Model 6 |\n",
    "|-|-|-|-|-|-|\n",
    "| 1-Day Return| Model 1 Vars.<br><br>Previous Action | Model 2 Vars.<br><br>Previous Price | Model 3 Vars.<br><br>2-Day Return<br><br>5-Day Return<br><br>10-Day Return<br><br>21-Day Return | Model 4 Vars.<br><br>2 Similar Indexes'<br>· 1-Day Return<br>· 5-Day Return<br>· 21-Day Return | Model 5 Vars.<br><br>2 Unconnected Indexes'<br>· 1-Day Return<br>· 5-Day Return<br>· 21-Day Return |\n",
    "\n",
    "This notebook allows you to specify which state space you want to use, as training all of them at once could be very time-consuming."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Settings\n",
    "<mark> Select which model you want to run by setting the model variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model variable\n",
    "#Which model to run(1 - 6, 0 being the simplest, 6 being the most complex)\n",
    "model = 1\n",
    "whenSave = 100 # after how many episodes should the csv be saved to google drive\n",
    "scale_test_prices = False # whether to scale the price state variable for models 3-6 to have the same range\n",
    "# as the training prices, them not having the same range lead to bad results in model 3\n",
    "trading_cost_bps = 0 #trading cost percent, i.e. .01 is a 1% trading cost\n",
    "time_cost_bps = 0 #time cost, deducted every day the agent doesn't do something different\n",
    "batch_size = 4096 # the batch size to train the NNs on\n",
    "max_episodes = 1000\n",
    "epsilon_decay_steps = 250 # how many steps it takes epsilon to go from 1 to .01\n",
    "\n",
    "\n",
    "#Random setup stuff\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "#Use a GPU is we have one\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "#Set up results directory to google drive\n",
    "results_path = \"/content/gdrive/My Drive/\"\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation Environment\n",
    "\n",
    "Our environment is a fairly simple market trading simulation. The agent has a choice of three actions:\n",
    "\n",
    ">A = E{0,1,2}, Sell Short, Flat, Buy Long\n",
    "\n",
    "Where\n",
    "* 0: Agent shorts the index fund equal to the amount of possessed capital.\n",
    "* 1: Agent transfers all possessed capital into cash and closes all short positions\n",
    "* 2: Agent buys as much of the given fund as possible with the possessed capital.\n",
    "\n",
    "This is a very simplistic model because the agent cannot invest only a portion of it's capital; it must invest all of its capital or none.\n",
    "\n",
    "At each time step, the simulation updates the portfolio's Net Asset Value (NAV), and performs the agent's chosen action. The NAV is calculated by the following formula:\n",
    "\n",
    "$$\n",
    "NAV_{new} = NAV_{old} * (1 + Reward)\n",
    "$$\n",
    "\n",
    "The function rewarding the agent is simply the percentage change of the NAV. The simulation uses the following equation\n",
    "to calculate the reward function:\n",
    "\n",
    "$$\n",
    "Reward = [(a_{n} - 1) * Return_{n+1}] – trading\\_cost\\_bps*|(a_{n} - 1) - (a_{n-1} - 1)| – dailyCost\n",
    "$$\n",
    "<br>\n",
    "where\n",
    "$$\n",
    "a_{n} = max(neuralNetwork.predict(S_{n})) ∈ {0, 1, 2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dailyCost =\n",
    "  \\begin{cases}\n",
    "    0 &\\text{if } a_{n} = a_{n-1}\\\\\\\n",
    "    \\text{time_cost_bps} &\\text{else }\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Where a<sub>n</sub> is the nth action, S<sub>n</sub> is the nth state, and Return<sub>n</sub> is the 1 day return of the market as found in S<sub>n</sub>.\n",
    "Actions are decremented by 1 so that it translates the action space to -1, 0, and 1. This way, if the agent held cash (now equal to 0), the 1-Day Return will not affect the NAV. If agent bought the stock (now equal\n",
    "to 1), the percent change of NAV will directly correlate to the 1-Day Return. And if the agent instead shorted, the percent change would be inversely correlated to the 1-Day Return.\n",
    "The number of trades are calculated by the equation |(a<sub>n</sub> - 1) - (a<sub>n-1</sub> - 1)|, because if both actions are the same,\n",
    " the agent doesn’t need to trade anything to execute the action. If the agent wants to buy and it previously\n",
    " only held cash, it would only have to buy |0 - 1| = 1 unit of the stock. Whereas if the agent wants to then\n",
    " short, it would first have to sell all the stock it already has and then short that amount again, resulting\n",
    " in |1 - (-1)| = 2 units of trading it would have to do. The amount needed to trade is then multiplied by the\n",
    " trading costs. The daily cost is used to disincentivize the agent from being too passive. If the agent\n",
    " repeats an action, the daily cost is set to the specified value, in our case 0.0001, but if the agent performs\n",
    " a different action, and thus trades something, the dailyCost will be set to 0.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Create and Initialize Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Simulation variables\n",
    "trading_days = 252 #days per episode\n",
    "#trading_cost_bps = 1e-3\n",
    "#time_cost_bps = 1e-4\n",
    "\n",
    "#create the gym environment using the trading_env.py file\n",
    "register(\n",
    "    id='trading-v0',\n",
    "    entry_point='trading_env:TradingEnvironment',\n",
    "    max_episode_steps=trading_days\n",
    ")\n",
    "\n",
    "#print environment variables\n",
    "f'Trading costs: {trading_cost_bps:.2%} | Time costs: {time_cost_bps:.2%}'\n",
    "\n",
    "#Initalize environment\n",
    "trading_environment = gym.make('trading-v0', trading_days = trading_days, model = model, scale_test_prices = scale_test_prices)\n",
    "trading_environment.env.trading_days = trading_days\n",
    "trading_environment.env.data_source.trading_days = trading_days\n",
    "trading_environment.env.simulator.steps = trading_days\n",
    "\n",
    "trading_environment.env.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.simulator.trading_cost_bps = trading_cost_bps\n",
    "trading_environment.env.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.simulator.time_cost_bps = time_cost_bps\n",
    "trading_environment.env.simulator.reinitialize()\n",
    "trading_environment.seed(42)\n",
    "\n",
    "# Get Environment Params\n",
    "state_dim = len(trading_environment.reset()) #the number of variables in the state space(the number of input nodes)\n",
    "num_actions = trading_environment.action_space.n #the number of actions(the number of output nodes)\n",
    "max_episode_steps = trading_environment.spec.max_episode_steps\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Define Trading Agent(the Neural Network)\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_dim,\n",
    "                 num_actions,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 replay_capacity,\n",
    "                 architecture,\n",
    "                 l2_reg,\n",
    "                 tau,\n",
    "                 batch_size):\n",
    "        #initialize\n",
    "        self.state_dim = state_dim #num input nodes\n",
    "        self.num_actions = num_actions #num output nodes\n",
    "        self.experience = deque([], maxlen=replay_capacity) #where to store data from training\n",
    "        self.learning_rate = learning_rate #NN learning rate\n",
    "        self.gamma = gamma #future rewards discount factor\n",
    "        self.architecture = architecture #archetecture of the NN\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.online_network = self.build_model() #The network to train\n",
    "        self.target_network = self.build_model(trainable=False) #the network to keep constant(helps converge quicker)\n",
    "        self.update_target() #set the weights of the target equal to the weights of the online network\n",
    "\n",
    "        self.epsilon = epsilon_start #percent chance to take a random action, vs the optimal action\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps # how many episodes for epsilon to go from epsilon_start to epsilon_end\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps #subtract this from epsilon each episode\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay #multiply epsilon by this each episode after epsilon_decay_steps number of episodes\n",
    "        self.epsilon_history = [] #store the epsilon values\n",
    "\n",
    "        self.total_steps = self.train_steps = 0 #keep track of how many steps we did\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0 #keep rack of the number of episodes\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.batch_size = batch_size #how many data points each round to train the network on\n",
    "        self.tau = tau # how many trainings until we update the target network with the online networks weights\n",
    "        self.losses = []\n",
    "        self.idx = tf.range(batch_size)\n",
    "        self.train = True\n",
    "\n",
    "    # create the neural network with keras\n",
    "    def build_model(self, trainable=True):\n",
    "        layers = []\n",
    "        n = len(self.architecture)\n",
    "        #add the dense layers\n",
    "        for i, units in enumerate(self.architecture, 1):\n",
    "            layers.append(Dense(units=units,\n",
    "                                input_dim=self.state_dim if i == 1 else None, #set the first input to be the dimention of the state space\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=l2(self.l2_reg),\n",
    "                                name=f'Dense_{i}',\n",
    "                                trainable=trainable))\n",
    "        #add a drop out layer\n",
    "        layers.append(Dropout(.1))\n",
    "        #add the final output layer\n",
    "        layers.append(Dense(units=self.num_actions,#number of actions\n",
    "                            trainable=trainable,\n",
    "                            name='Output'))\n",
    "        #initialize the model\n",
    "        model = Sequential(layers)\n",
    "        #add the MSE loss and adam optimizer\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    #set the target network's weight equal to the online network's weights\n",
    "    def update_target(self):\n",
    "        self.target_network.set_weights(self.online_network.get_weights())\n",
    "\n",
    "    #Choose an action based on e greedy\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps += 1\n",
    "        #check if we choose a random action or optimal action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)#return a random action\n",
    "        #return the optimal action\n",
    "        q = self.online_network.predict(state)#predict the q values for each action\n",
    "        return np.argmax(q, axis=1).squeeze()#return the action with the highest Q value\n",
    "\n",
    "    #store the state, action reward, and next state\n",
    "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
    "        #check if its the end of the episode\n",
    "        if not_done:\n",
    "            #it is not so add to the episode\n",
    "            self.episode_reward += r\n",
    "            self.episode_length += 1\n",
    "        else:\n",
    "            #episode ended so decrease epsilon\n",
    "            if self.train:\n",
    "                #check if epsilon needs to be decreased incrementally or exponentially\n",
    "                if self.episodes < self.epsilon_decay_steps:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                else:\n",
    "                    self.epsilon *= self.epsilon_exponential_decay\n",
    "            #add stuff to historic storage\n",
    "            self.episodes += 1\n",
    "            self.rewards_history.append(self.episode_reward)\n",
    "            self.steps_per_episode.append(self.episode_length)\n",
    "            self.episode_reward, self.episode_length = 0, 0\n",
    "        #add the pairs\n",
    "        self.experience.append((s, a, r, s_prime, not_done))\n",
    "\n",
    "    #train the neural networks\n",
    "    def experience_replay(self):\n",
    "        #make sure we have enough data\n",
    "        if self.batch_size > len(self.experience):\n",
    "            return\n",
    "        #Get a batch of data\n",
    "        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n",
    "        states, actions, rewards, next_states, not_done = minibatch\n",
    "        #get the predicted bes actions for the next_states\n",
    "        next_q_values = self.online_network.predict_on_batch(next_states)\n",
    "        best_actions = tf.argmax(next_q_values, axis=1)\n",
    "        #get the predicted reward for the future states from the target network\n",
    "        next_q_values_target = self.target_network.predict_on_batch(next_states)\n",
    "        target_q_values = tf.gather_nd(next_q_values_target,\n",
    "                                       tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n",
    "        #the expected q_values are equal to the observed reward + gamma*the predicted reward of the next state, gamma being the future discount factor\n",
    "        targets = rewards + not_done * self.gamma * target_q_values\n",
    "        #get the predicted q-values, and the store the expected q_values we have among them\n",
    "        q_values = self.online_network.predict_on_batch(states)\n",
    "        q_values[[self.idx, actions]] = targets\n",
    "        #train the network to get q_values when states are passed in, and store the loss\n",
    "        loss = self.online_network.train_on_batch(x=states, y=q_values)\n",
    "        self.losses.append(loss)\n",
    "        #check if we need to set the targets weights to the onlines weights\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reinforcement Learning parameters\n",
    "\n",
    "gamma = .99,  # discount factor\n",
    "tau = 100  # target network update frequency\n",
    "\n",
    "# Neural Network Architecture\n",
    "\n",
    "architecture = (256, 256)  # units per layer\n",
    "learning_rate = 0.0001  # learning rate\n",
    "l2_reg = 1e-6  # L2 regularization\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "replay_capacity = int(1e6)\n",
    "#batch_size = 4096\n",
    "\n",
    "### epsilon-greedy Policy\n",
    "\n",
    "epsilon_start = 1.0 # starting point for epsilon\n",
    "epsilon_end = .01 # ending point for epsilon\n",
    "#epsilon_decay_steps = 250 # the number of steps to get from start to end\n",
    "\n",
    "epsilon_exponential_decay = .99 # after 250 step(epsilon_decay_steps) epsilon = epsilon*epsilon_exponential_decay"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create DDQN Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clear out karas\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Instantiate the DDQN model\n",
    "ddqn = DDQNAgent(state_dim=state_dim,\n",
    "                 num_actions=num_actions,\n",
    "                 learning_rate=learning_rate,\n",
    "                 gamma=gamma,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                 replay_capacity=replay_capacity,\n",
    "                 architecture=architecture,\n",
    "                 l2_reg=l2_reg,\n",
    "                 tau=tau,\n",
    "                 batch_size=batch_size)\n",
    "\n",
    "ddqn.online_network.summary()\n",
    "\n",
    "### Set Experiment parameters\n",
    "\n",
    "total_steps = 0\n",
    "#max_episodes = 1000\n",
    "\n",
    "### Initialize Experiment variables\n",
    "# these store all the data we want to keep track of\n",
    "# navs - agent's net asset values for each episode\n",
    "# market_navs- market's net asset values for each episode\n",
    "# diffs- navs - market_navs\n",
    "# holds - the number of times the agent held\n",
    "# shorts - the number of times the agent shorted\n",
    "# buys - the number of times the agent bought\n",
    "# stds - the standard deviations for the market and agents, so we can calculate the sharpe ratio\n",
    "episode_time, navs, market_navs, diffs, holds, shorts, buys = [], [], [], [], [], [], []\n",
    "test_navs, test_market_navs, test_diffs, test_holds, test_shorts, test_buys = [], [], [], [], [], []\n",
    "trainM_std, trainA_std, testA_std, testM_std = [], [], [], []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Prints the results from the training and testing runs\n",
    "\n",
    "def track_results(episode, nav_ma_100, nav_ma_10,\n",
    "                  market_nav_100, market_nav_10,\n",
    "                  win_ratio, total, epsilon, pretext=\"Training Results:\"):\n",
    "    time_ma = np.mean([episode_time[-100:]])\n",
    "    T = np.sum(episode_time)\n",
    "    #set up the format to print to\n",
    "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
    "    #print\n",
    "    print(pretext + template.format(episode, format_time(total),\n",
    "                          nav_ma_100-1, nav_ma_10-1,\n",
    "                          market_nav_100-1, market_nav_10-1,\n",
    "                          win_ratio, epsilon))\n",
    "\n",
    "#Runs a year long simulation on the testing data, does not train\n",
    "def test_data_simulation():\n",
    "    #reset the environment\n",
    "    testthis_state = trading_environment.reset(training=False)\n",
    "    num_holds = 0\n",
    "    num_buys = 0\n",
    "    num_shorts = 0\n",
    "\n",
    "    #loop for a year\n",
    "    for test_episode_step in range(max_episode_steps):\n",
    "        #get action\n",
    "        testaction = ddqn.epsilon_greedy_policy(testthis_state.reshape(-1, state_dim))\n",
    "        #do action and get result\n",
    "        testnext_state, testreward, testdone, _ = trading_environment.step(testaction)\n",
    "\n",
    "        #store what action was performed\n",
    "        if testaction == 0:\n",
    "            num_shorts += 1\n",
    "        elif testaction == 1:\n",
    "            num_holds += 1\n",
    "        else:\n",
    "            num_buys += 1\n",
    "\n",
    "\n",
    "        if testdone:\n",
    "            break\n",
    "        #reset the state\n",
    "        testthis_state = testnext_state\n",
    "\n",
    "\n",
    "\n",
    "    #add the data to the lists\n",
    "    test_holds.append(num_holds)\n",
    "    test_shorts.append(num_shorts)\n",
    "    test_buys.append(num_buys)\n",
    "    testM_std.append(np.std(trading_environment.env.simulator.market_returns))\n",
    "    testA_std.append(np.std(trading_environment.env.simulator.strategy_returns))\n",
    "\n",
    "    # get DataFrame with sequence of actions, returns, and NAV values\n",
    "    test_result = trading_environment.env.simulator.result()\n",
    "\n",
    "    # get results of last step\n",
    "    test_final = test_result.iloc[-1]\n",
    "\n",
    "\n",
    "    # get nav\n",
    "    test_nav = test_final.nav\n",
    "    test_navs.append(test_nav)\n",
    "\n",
    "    # market nav\n",
    "    test_market_nav = test_final.market_nav\n",
    "    test_market_navs.append(test_market_nav)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    test_diff = test_nav - test_market_nav\n",
    "    test_diffs.append(test_diff)\n",
    "\n",
    "    #print the results\n",
    "    track_results(episode,\n",
    "                  # show mov. average results for 100 (10) periods\n",
    "                  np.mean(test_navs[-100:]),\n",
    "                  np.mean(test_navs[-10:]),\n",
    "                  np.mean(test_market_navs[-100:]),\n",
    "                  np.mean(test_market_navs[-10:]),\n",
    "                  # share of agent wins, defined as higher ending NAV\n",
    "                  np.sum([s > 0 for s in test_diffs[-100:]])/min(len(test_diffs), 100),\n",
    "                  time() - start, -1, pretext=\"Testing Results:\")\n",
    "\n",
    "#saves the data to a csv file in google drive\n",
    "def saveData():\n",
    "    print(len(diffs))\n",
    "\n",
    "    #put the data in a dataframe\n",
    "    results = pd.DataFrame({'NumStateVars': state_dim,\n",
    "                            'TradeCost': trading_cost_bps,\n",
    "                            'TimeCost': time_cost_bps,\n",
    "                            'EpsilonSteps': epsilon_decay_steps,\n",
    "                            'Episode': list(range(1, episode+1)),\n",
    "                            'TrainAgent': navs,\n",
    "                            'TrainMarket': market_navs,\n",
    "                            'TrainDifference': diffs,\n",
    "                            'Holds': holds,\n",
    "                            'Buys': buys,\n",
    "                            'Shorts': shorts,\n",
    "                            'AStd': trainA_std,\n",
    "                            'MStd': trainM_std}).set_index('Episode')\n",
    "    #get the win percent of the past 100 episodes\n",
    "    results['Strategy Wins (%)'] = (results.TrainDifference > 0).rolling(100).sum()\n",
    "\n",
    "    #store the test data in a dataframe\n",
    "    test_results = pd.DataFrame({'NumStateVars': state_dim,\n",
    "                            'TradeCost': trading_cost_bps,\n",
    "                            'TimeCost': time_cost_bps,\n",
    "                            'EpsilonSteps': epsilon_decay_steps,\n",
    "                            'EpisodeDiv10': list(range(1, len(test_navs)+1)),\n",
    "                            'TestAgent': test_navs,\n",
    "                            'TestMarket': test_market_navs,\n",
    "                            'TestDifference': test_diffs,\n",
    "                            'Holds': test_holds,\n",
    "                            'Buys': test_buys,\n",
    "                            'Shorts': test_shorts,\n",
    "                            'AStd': testA_std,\n",
    "                            'MStd': testM_std,\n",
    "                            'scaleTestPrices': scale_test_prices}).set_index('EpisodeDiv10')\n",
    "\n",
    "    #get the win percent\n",
    "    test_results['Strategy Wins (%)'] = (test_results.TestDifference > 0).rolling(100).sum()\n",
    "\n",
    "    # get the date and time so we can keep an ordered record of the data files\n",
    "    currentTime = datetime.now()\n",
    "    training_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TrainResults.csv'\n",
    "    testing_file_name = currentTime.strftime(\"%Y-%m-%d-%H%M-\") + 'TestResults.csv'\n",
    "\n",
    "\n",
    "    # store the results in a csv file\n",
    "    results.to_csv(results_path + training_file_name)\n",
    "    test_results.to_csv(results_path + testing_file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Agent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print stuff out\n",
    "start = time()\n",
    "results = []\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"model: \", model)\n",
    "print(\"scale_test_prices: \", scale_test_prices)\n",
    "print(\"trading_cost_bps: \", trading_cost_bps)\n",
    "print(\"time_cost_bps: \", time_cost_bps)\n",
    "print(\"batch_size: \", batch_size)\n",
    "print(\"max_episodes: \", max_episodes)\n",
    "print(\"epsilon_decay_steps: \", epsilon_decay_steps)\n",
    "print(\"-----------------------------------------------\")\n",
    "for episode in range(1, max_episodes + 1):\n",
    "    #reset the environment and get the first state\n",
    "    this_state = trading_environment.reset()\n",
    "    numBuy = 0\n",
    "    numShort = 0\n",
    "    numHold = 0\n",
    "    print(\"Episode: \", episode)\n",
    "    #loop for a year\n",
    "    for episode_step in range(max_episode_steps):\n",
    "        #get the action chosen by the agent\n",
    "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "\n",
    "        #store the action\n",
    "        if action == 0:\n",
    "            numShort += 1\n",
    "        elif action == 1:\n",
    "            numHold += 1\n",
    "        else:\n",
    "            numBuy += 1\n",
    "\n",
    "        #execute the action and get the results\n",
    "        next_state, reward, done, _ = trading_environment.step(action)\n",
    "        #store the results\n",
    "        ddqn.memorize_transition(this_state,\n",
    "                                 action,\n",
    "                                 reward,\n",
    "                                 next_state,\n",
    "                                 0.0 if done else 1.0)#whether or not the episode is over\n",
    "\n",
    "        #train the network\n",
    "        if ddqn.train:\n",
    "            ddqn.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "        #increment state\n",
    "        this_state = next_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # get DataFrame with sequence of actions, returns and NAV values\n",
    "    result = trading_environment.env.simulator.result()\n",
    "\n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # get NAV\n",
    "    nav = final.nav\n",
    "    navs.append(nav)\n",
    "\n",
    "    # market NAV\n",
    "    market_nav = final.market_nav\n",
    "    market_navs.append(market_nav)\n",
    "\n",
    "    #num holds buys and sells\n",
    "    holds.append(numHold)\n",
    "    buys.append(numBuy)\n",
    "    shorts.append(numShort)\n",
    "    #stds\n",
    "    trainM_std.append(np.std(trading_environment.env.simulator.market_returns))\n",
    "    trainA_std.append(np.std(trading_environment.env.simulator.strategy_returns))\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    diff = nav - market_nav\n",
    "    diffs.append(diff)\n",
    "    if episode % 10 == 0:\n",
    "        track_results(episode,\n",
    "                      # show mov. average results for 100 (10) periods\n",
    "\n",
    "                      np.mean(navs[-100:]),\n",
    "                      np.mean(navs[-10:]),\n",
    "                      np.mean(market_navs[-100:]),\n",
    "                      np.mean(market_navs[-10:]),\n",
    "                      # share of agent wins, defined as higher ending NAV\n",
    "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
    "                      time() - start, ddqn.epsilon)\n",
    "        #run the model on testing data\n",
    "        test_data_simulation()\n",
    "    #save the data periodicaly, incase it crashes so we don't loose everything\n",
    "    if episode % whenSave == 0:\n",
    "        saveData()\n",
    "\n",
    "\n",
    "print(\"final\")\n",
    "#print results\n",
    "track_results(episode,\n",
    "              # show mov. average results for 100 (10) periods\n",
    "              np.mean(navs[-100:]),\n",
    "              np.mean(navs[-10:]),\n",
    "              np.mean(market_navs[-100:]),\n",
    "              np.mean(market_navs[-10:]),\n",
    "              # share of agent wins, defined as higher ending nav\n",
    "              np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
    "              time() - start, ddqn.epsilon)\n",
    "#run everything one last time to get final results\n",
    "test_data_simulation()\n",
    "trading_environment.close()\n",
    "saveData()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Store Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "saveData()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# plot density histogram\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    sns.distplot(results.TrainDifference)\n",
    "    sns.despine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title\n",
    "# plot annual returns and agent outperformance line graphs\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "df1 = (results[['TrainAgent', 'TrainMarket']]\n",
    "       .sub(1)\n",
    "       .rolling(100)\n",
    "       .mean())\n",
    "df1.plot(ax=axes[0],\n",
    "         title='Annual Returns (Moving Average)',\n",
    "         lw=1)\n",
    "\n",
    "df2 = results['Strategy Wins (%)'].div(100).rolling(50).mean()\n",
    "df2.plot(ax=axes[1],\n",
    "         title='Agent Outperformance (%, Moving Average)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.yaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))\n",
    "axes[1].axhline(.5, ls='--', c='k', lw=1)\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path / 'performance', dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}